# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2023, MLX Contributors
# This file is distributed under the same license as the MLX package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: MLX 0.2\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-12-08 12:55+0000\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"Language: zh_TW\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"

#: ../../../src/usage/unified_memory.rst:4
msgid "Unified Memory"
msgstr "統一記憶體"

#: ../../../src/usage/unified_memory.rst:8
msgid ""
"Apple silicon has a unified memory architecture. The CPU and GPU have direct "
"access to the same memory pool. MLX is designed to take advantage of that."
msgstr ""
"Apple silicon 採用統一記憶體架構。CPU 與 GPU 可直接存取同一個記憶體池。MLX 的"
"設計即是為了善用這項特性。"

#: ../../../src/usage/unified_memory.rst:11
msgid ""
"Concretely, when you make an array in MLX you don't have to specify its "
"location:"
msgstr "更具體地說，在 MLX 中建立陣列時不需要指定其位置："

#: ../../../src/usage/unified_memory.rst:14
msgid ""
"a = mx.random.normal((100,))\n"
"b = mx.random.normal((100,))"
msgstr ""
"a = mx.random.normal((100,))\n"
"b = mx.random.normal((100,))"

#: ../../../src/usage/unified_memory.rst:19
msgid "Both ``a`` and ``b`` live in unified memory."
msgstr "``a`` 和 ``b`` 皆存在於統一記憶體中。"

#: ../../../src/usage/unified_memory.rst:21
msgid ""
"In MLX, rather than moving arrays to devices, you specify the device when "
"you run the operation. Any device can perform any operation on ``a`` and "
"``b`` without needing to move them from one memory location to another. For "
"example:"
msgstr ""
"在 MLX 中，你不需要把陣列搬移到裝置上，而是在執行運算時指定裝置。任何裝置都能"
"在 ``a`` 與 ``b`` 上執行任何運算，而不必在不同記憶體位置間搬移。例如："

#: ../../../src/usage/unified_memory.rst:25
msgid ""
"mx.add(a, b, stream=mx.cpu)\n"
"mx.add(a, b, stream=mx.gpu)"
msgstr ""
"mx.add(a, b, stream=mx.cpu)\n"
"mx.add(a, b, stream=mx.gpu)"

#: ../../../src/usage/unified_memory.rst:30
msgid ""
"In the above, both the CPU and the GPU will perform the same add operation. "
"The operations can (and likely will) be run in parallel since there are no "
"dependencies between them. See :ref:`using_streams` for more information the "
"semantics of streams in MLX."
msgstr ""
"在上例中，CPU 與 GPU 都會執行相同的加法運算。因為兩者之間沒有相依性，所以這些"
"運算可以（而且很可能會）並行執行。關於 MLX 中串流語意的更多資訊，請參考 :ref:"
"`using_streams`。"

#: ../../../src/usage/unified_memory.rst:35
msgid ""
"In the above ``add`` example, there are no dependencies between operations, "
"so there is no possibility for race conditions. If there are dependencies, "
"the MLX scheduler will automatically manage them. For example:"
msgstr ""
"在上述 ``add`` 範例中，運算之間沒有相依性，因此不會產生競態條件。若存在相依"
"性，MLX 排程器會自動管理。例如："

#: ../../../src/usage/unified_memory.rst:39
msgid ""
"c = mx.add(a, b, stream=mx.cpu)\n"
"d = mx.add(a, c, stream=mx.gpu)"
msgstr ""
"c = mx.add(a, b, stream=mx.cpu)\n"
"d = mx.add(a, c, stream=mx.gpu)"

#: ../../../src/usage/unified_memory.rst:44
msgid ""
"In the above case, the second ``add`` runs on the GPU but it depends on the "
"output of the first ``add`` which is running on the CPU. MLX will "
"automatically insert a dependency between the two streams so that the second "
"``add`` only starts executing after the first is complete and ``c`` is "
"available."
msgstr ""
"在上述情況下，第二個 ``add`` 在 GPU 上執行，但它相依於第一個在 CPU 上執行的 "
"``add`` 的輸出。MLX 會自動在兩個串流間插入相依性，讓第二個 ``add`` 只有在第一"
"個完成且 ``c`` 可用後才開始執行。"

#: ../../../src/usage/unified_memory.rst:51
msgid "A Simple Example"
msgstr "簡單範例"

#: ../../../src/usage/unified_memory.rst:53
msgid ""
"Here is a more interesting (albeit slightly contrived example) of how "
"unified memory can be helpful. Suppose we have the following computation:"
msgstr ""
"以下是更有趣（雖然稍微刻意）的例子，說明統一記憶體如何提供幫助。假設我們有以"
"下計算："

#: ../../../src/usage/unified_memory.rst:56
msgid ""
"def fun(a, b, d1, d2):\n"
"  x = mx.matmul(a, b, stream=d1)\n"
"  for _ in range(500):\n"
"      b = mx.exp(b, stream=d2)\n"
"  return x, b"
msgstr ""
"def fun(a, b, d1, d2):\n"
"  x = mx.matmul(a, b, stream=d1)\n"
"  for _ in range(500):\n"
"      b = mx.exp(b, stream=d2)\n"
"  return x, b"

#: ../../../src/usage/unified_memory.rst:64
msgid "which we want to run with the following arguments:"
msgstr "我們希望使用下列參數執行："

#: ../../../src/usage/unified_memory.rst:66
msgid ""
"a = mx.random.uniform(shape=(4096, 512))\n"
"b = mx.random.uniform(shape=(512, 4))"
msgstr ""
"a = mx.random.uniform(shape=(4096, 512))\n"
"b = mx.random.uniform(shape=(512, 4))"

#: ../../../src/usage/unified_memory.rst:71
msgid ""
"The first ``matmul`` operation is a good fit for the GPU since it's more "
"compute dense. The second sequence of operations are a better fit for the "
"CPU, since they are very small and would probably be overhead bound on the "
"GPU."
msgstr ""
"第一個 ``matmul`` 運算計算密度高，適合在 GPU 上執行。第二段運算序列非常小，"
"在 GPU 上可能受限於開銷，因此更適合在 CPU 上執行。"

#: ../../../src/usage/unified_memory.rst:75
msgid ""
"If we time the computation fully on the GPU, we get 2.8 milliseconds. But if "
"we run the computation with ``d1=mx.gpu`` and ``d2=mx.cpu``, then the time "
"is only about 1.4 milliseconds, about twice as fast. These times were "
"measured on an M1 Max."
msgstr ""
"如果完全在 GPU 上計時，耗時為 2.8 毫秒。但若以 ``d1=mx.gpu``、``d2=mx.cpu`` "
"執行，時間約為 1.4 毫秒，快了約兩倍。這些數據是在 M1 Max 上測得。"
