# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2023, MLX Contributors
# This file is distributed under the same license as the MLX package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: MLX 0.0\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-12-08 12:58+0000\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"Language: zh_TW\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"

#: ../../../src/python/_autosummary/mlx.optimizers.AdaDelta.rst:2
msgid "mlx.optimizers.AdaDelta"
msgstr ""

#: ../../../../../../.conda/lib/python3.12/site-packages/mlx/optimizers.py:docstring
#: of mlx.optimizers.AdaDelta:1
msgid "The AdaDelta optimizer with a learning rate [1]."
msgstr ""

#: ../../../../../../.conda/lib/python3.12/site-packages/mlx/optimizers.py:docstring
#: of mlx.optimizers.AdaDelta:3
msgid "Our AdaDelta implementation follows the original paper. In detail,"
msgstr ""

#: ../../../../../../.conda/lib/python3.12/site-packages/mlx/optimizers.py:docstring
#: of mlx.optimizers.AdaDelta:5
msgid ""
"[1]: Zeiler, M.D., 2012. ADADELTA: an adaptive learning rate method. arXiv "
"preprint arXiv:1212.5701."
msgstr ""

#: ../../../../../../.conda/lib/python3.12/site-packages/mlx/optimizers.py:docstring
#: of mlx.optimizers.AdaDelta:7
msgid ""
"v_{t+1} &= \\rho v_t + (1 - \\rho) g_t^2 \\\\\n"
"\\Delta w_{t+1} &= \\frac{\\sqrt{u_t + \\epsilon}}{\\sqrt{v_{t+1} + "
"\\epsilon}} g_t \\\\\n"
"u_{t+1} &= \\rho u_t + (1 - \\rho) \\Delta w_{t+1}^2 \\\\\n"
"w_{t+1} &= w_t - \\lambda \\Delta w_{t+1}"
msgstr ""

#: ../../../src/python/_autosummary/mlx.optimizers.AdaDelta.rst:0
msgid "Parameters"
msgstr ""

#: ../../../../../../.conda/lib/python3.12/site-packages/mlx/optimizers.py:docstring
#: of mlx.optimizers.AdaDelta:14
msgid "The learning rate :math:`\\lambda`."
msgstr ""

#: ../../../../../../.conda/lib/python3.12/site-packages/mlx/optimizers.py:docstring
#: of mlx.optimizers.AdaDelta:16
msgid ""
"The coefficient :math:`\\rho` used for computing a running average of "
"squared gradients. Default: ``0.9``"
msgstr ""

#: ../../../../../../.conda/lib/python3.12/site-packages/mlx/optimizers.py:docstring
#: of mlx.optimizers.AdaDelta:19
msgid ""
"The term :math:`\\epsilon` added to the denominator to improve numerical "
"stability. Default: `1e-8`"
msgstr ""

#: ../../../src/python/_autosummary/mlx.optimizers.AdaDelta.rst:12
msgid "Methods"
msgstr ""

#: ../../../src/python/_autosummary/mlx.optimizers.AdaDelta.rst:16:<autosummary>:1
msgid ""
":py:obj:`__init__ <mlx.optimizers.AdaDelta.__init__>`\\ "
"\\(learning\\_rate\\[\\, rho\\, eps\\]\\)"
msgstr ""

#: ../../../src/python/_autosummary/mlx.optimizers.AdaDelta.rst:16:<autosummary>:1
msgid ""
":py:obj:`apply_single <mlx.optimizers.AdaDelta.apply_single>`\\ "
"\\(gradient\\, parameter\\, state\\)"
msgstr ""

#: ../../../src/python/_autosummary/mlx.optimizers.AdaDelta.rst:16:<autosummary>:1
msgid ""
"Performs the AdaDelta parameter update and stores :math:`v` and :math:`u` in "
"the optimizer state."
msgstr ""
