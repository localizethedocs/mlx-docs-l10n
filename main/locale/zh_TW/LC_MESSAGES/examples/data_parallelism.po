# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2023, Apple
# This file is distributed under the same license as the MLX package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: MLX main\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2026-02-16 08:39+0000\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"Language: zh_TW\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"

#: ../../../src/examples/data_parallelism.rst:4
msgid "Data Parallelism"
msgstr ""

#: ../../../src/examples/data_parallelism.rst:6
msgid ""
"MLX enables efficient data parallel distributed training through its "
"distributed communication primitives."
msgstr ""

#: ../../../src/examples/data_parallelism.rst:12
msgid "Training Example"
msgstr ""

#: ../../../src/examples/data_parallelism.rst:14
msgid ""
"In this section we will adapt an MLX training loop to support data parallel "
"distributed training. Namely, we will average the gradients across a set of "
"hosts before applying them to the model."
msgstr ""

#: ../../../src/examples/data_parallelism.rst:18
msgid ""
"Our training loop looks like the following code snippet if we omit the "
"model, dataset, and optimizer initialization."
msgstr ""

#: ../../../src/examples/data_parallelism.rst:21
msgid ""
"model = ...\n"
"optimizer = ...\n"
"dataset = ...\n"
"\n"
"def step(model, x, y):\n"
"    loss, grads = loss_grad_fn(model, x, y)\n"
"    optimizer.update(model, grads)\n"
"    return loss\n"
"\n"
"for x, y in dataset:\n"
"    loss = step(model, x, y)\n"
"    mx.eval(loss, model.parameters())"
msgstr ""

#: ../../../src/examples/data_parallelism.rst:36
msgid ""
"All we have to do to average the gradients across machines is perform an :"
"func:`all_sum` and divide by the size of the :class:`Group`. Namely we have "
"to :func:`mlx.utils.tree_map` the gradients with following function."
msgstr ""

#: ../../../src/examples/data_parallelism.rst:40
msgid ""
"def all_avg(x):\n"
"    return mx.distributed.all_sum(x) / mx.distributed.init().size()"
msgstr ""

#: ../../../src/examples/data_parallelism.rst:45
msgid ""
"Putting everything together our training loop step looks as follows with "
"everything else remaining the same."
msgstr ""

#: ../../../src/examples/data_parallelism.rst:48
msgid ""
"from mlx.utils import tree_map\n"
"\n"
"def all_reduce_grads(grads):\n"
"    N = mx.distributed.init().size()\n"
"    if N == 1:\n"
"        return grads\n"
"    return tree_map(\n"
"        lambda x: mx.distributed.all_sum(x) / N,\n"
"        grads\n"
"    )\n"
"\n"
"def step(model, x, y):\n"
"    loss, grads = loss_grad_fn(model, x, y)\n"
"    grads = all_reduce_grads(grads)  # <--- This line was added\n"
"    optimizer.update(model, grads)\n"
"    return loss"
msgstr ""

#: ../../../src/examples/data_parallelism.rst:68
msgid "Using ``nn.average_gradients``"
msgstr ""

#: ../../../src/examples/data_parallelism.rst:70
msgid ""
"Although the code example above works correctly; it performs one "
"communication per gradient. It is significantly more efficient to aggregate "
"several gradients together and perform fewer communication steps."
msgstr ""

#: ../../../src/examples/data_parallelism.rst:74
msgid ""
"This is the purpose of :func:`mlx.nn.average_gradients`. The final code "
"looks almost identical to the example above:"
msgstr ""

#: ../../../src/examples/data_parallelism.rst:77
msgid ""
"model = ...\n"
"optimizer = ...\n"
"dataset = ...\n"
"\n"
"def step(model, x, y):\n"
"    loss, grads = loss_grad_fn(model, x, y)\n"
"    grads = mx.nn.average_gradients(grads)  # <---- This line was added\n"
"    optimizer.update(model, grads)\n"
"    return loss\n"
"\n"
"for x, y in dataset:\n"
"    loss = step(model, x, y)\n"
"    mx.eval(loss, model.parameters())"
msgstr ""
