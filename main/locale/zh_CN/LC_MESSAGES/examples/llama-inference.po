# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2023, Apple
# This file is distributed under the same license as the MLX package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: MLX main\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-12-08 12:09+0000\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"Language: zh_CN\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"

#: ../../../src/examples/llama-inference.rst:2
msgid "LLM inference"
msgstr ""

#: ../../../src/examples/llama-inference.rst:4
msgid ""
"MLX enables efficient inference of large-ish transformers on Apple silicon "
"without compromising on ease of use. In this example we will create an "
"inference script for the Llama family of transformer models in which the "
"model is defined in less than 200 lines of python."
msgstr ""

#: ../../../src/examples/llama-inference.rst:10
msgid "Implementing the model"
msgstr ""

#: ../../../src/examples/llama-inference.rst:12
msgid ""
"We will use the neural network building blocks defined in the :mod:`mlx.nn` "
"module to concisely define the model architecture."
msgstr ""

#: ../../../src/examples/llama-inference.rst:16
msgid "Attention layer"
msgstr ""

#: ../../../src/examples/llama-inference.rst:18
msgid ""
"We will start with the Llama attention layer which notably uses the RoPE "
"positional encoding. [1]_ In addition, our attention layer will optionally "
"use a key/value cache that will be concatenated with the provided keys and "
"values to support efficient inference."
msgstr ""

#: ../../../src/examples/llama-inference.rst:23
msgid ""
"Our implementation uses :class:`mlx.nn.Linear` for all the projections and :"
"class:`mlx.nn.RoPE` for the positional encoding."
msgstr ""

#: ../../../src/examples/llama-inference.rst:26
msgid ""
"import mlx.core as mx\n"
"import mlx.nn as nn\n"
"\n"
"class LlamaAttention(nn.Module):\n"
"    def __init__(self, dims: int, num_heads: int):\n"
"        super().__init__()\n"
"\n"
"        self.num_heads = num_heads\n"
"\n"
"        self.rope = nn.RoPE(dims // num_heads, traditional=True)\n"
"        self.query_proj = nn.Linear(dims, dims, bias=False)\n"
"        self.key_proj = nn.Linear(dims, dims, bias=False)\n"
"        self.value_proj = nn.Linear(dims, dims, bias=False)\n"
"        self.out_proj = nn.Linear(dims, dims, bias=False)\n"
"\n"
"    def __call__(self, queries, keys, values, mask=None, cache=None):\n"
"        queries = self.query_proj(queries)\n"
"        keys = self.key_proj(keys)\n"
"        values = self.value_proj(values)\n"
"\n"
"        # Extract some shapes\n"
"        num_heads = self.num_heads\n"
"        B, L, D = queries.shape\n"
"\n"
"        # Prepare the queries, keys and values for the attention "
"computation\n"
"        queries = queries.reshape(B, L, num_heads, -1).transpose(0, 2, 1, "
"3)\n"
"        keys = keys.reshape(B, L, num_heads, -1).transpose(0, 2, 1, 3)\n"
"        values = values.reshape(B, L, num_heads, -1).transpose(0, 2, 1, 3)\n"
"\n"
"        # Add RoPE to the queries and keys and combine them with the cache\n"
"        if cache is not None:\n"
"            key_cache, value_cache = cache\n"
"            queries = self.rope(queries, offset=key_cache.shape[2])\n"
"            keys = self.rope(keys, offset=key_cache.shape[2])\n"
"            keys = mx.concatenate([key_cache, keys], axis=2)\n"
"            values = mx.concatenate([value_cache, values], axis=2)\n"
"        else:\n"
"            queries = self.rope(queries)\n"
"            keys = self.rope(keys)\n"
"\n"
"        # Finally perform the attention computation\n"
"        scale = math.sqrt(1 / queries.shape[-1])\n"
"        scores = (queries * scale) @ keys.transpose(0, 1, 3, 2)\n"
"        if mask is not None:\n"
"            scores = scores + mask\n"
"        scores = mx.softmax(scores, axis=-1)\n"
"        values_hat = (scores @ values).transpose(0, 2, 1, 3).reshape(B, L, "
"-1)\n"
"\n"
"        # Note that we return the keys and values to possibly be used as a "
"cache\n"
"        return self.out_proj(values_hat), (keys, values)"
msgstr ""

#: ../../../src/examples/llama-inference.rst:80
msgid "Encoder layer"
msgstr ""

#: ../../../src/examples/llama-inference.rst:82
msgid ""
"The other component of the Llama model is the encoder layer which uses RMS "
"normalization [2]_ and SwiGLU. [3]_ For RMS normalization we will use :class:"
"`mlx.nn.RMSNorm` that is already provided in :mod:`mlx.nn`."
msgstr ""

#: ../../../src/examples/llama-inference.rst:86
msgid ""
"class LlamaEncoderLayer(nn.Module):\n"
"    def __init__(self, dims: int, mlp_dims: int, num_heads: int):\n"
"        super().__init__()\n"
"\n"
"        self.attention = LlamaAttention(dims, num_heads)\n"
"\n"
"        self.norm1 = nn.RMSNorm(dims)\n"
"        self.norm2 = nn.RMSNorm(dims)\n"
"\n"
"        self.linear1 = nn.Linear(dims, mlp_dims, bias=False)\n"
"        self.linear2 = nn.Linear(dims, mlp_dims, bias=False)\n"
"        self.linear3 = nn.Linear(mlp_dims, dims, bias=False)\n"
"\n"
"    def __call__(self, x, mask=None, cache=None):\n"
"        y = self.norm1(x)\n"
"        y, cache = self.attention(y, y, y, mask, cache)\n"
"        x = x + y\n"
"\n"
"        y = self.norm2(x)\n"
"        a = self.linear1(y)\n"
"        b = self.linear2(y)\n"
"        y = a * mx.sigmoid(a) * b\n"
"        y = self.linear3(y)\n"
"        x = x + y\n"
"\n"
"        return x, cache"
msgstr ""

#: ../../../src/examples/llama-inference.rst:116
msgid "Full model"
msgstr ""

#: ../../../src/examples/llama-inference.rst:118
msgid ""
"To implement any Llama model we simply have to combine ``LlamaEncoderLayer`` "
"instances with an :class:`mlx.nn.Embedding` to embed the input tokens."
msgstr ""

#: ../../../src/examples/llama-inference.rst:121
msgid ""
"class Llama(nn.Module):\n"
"    def __init__(\n"
"        self, num_layers: int, vocab_size: int, dims: int, mlp_dims: int, "
"num_heads: int\n"
"    ):\n"
"        super().__init__()\n"
"\n"
"        self.embedding = nn.Embedding(vocab_size, dims)\n"
"        self.layers = [\n"
"            LlamaEncoderLayer(dims, mlp_dims, num_heads) for _ in "
"range(num_layers)\n"
"        ]\n"
"        self.norm = nn.RMSNorm(dims)\n"
"        self.out_proj = nn.Linear(dims, vocab_size, bias=False)\n"
"\n"
"    def __call__(self, x):\n"
"        mask = nn.MultiHeadAttention.create_additive_causal_mask(x."
"shape[1])\n"
"        mask = mask.astype(self.embedding.weight.dtype)\n"
"\n"
"        x = self.embedding(x)\n"
"        for l in self.layers:\n"
"            x, _ = l(x, mask)\n"
"        x = self.norm(x)\n"
"        return self.out_proj(x)"
msgstr ""

#: ../../../src/examples/llama-inference.rst:146
msgid ""
"Note that in the implementation above we use a simple list to hold the "
"encoder layers but using ``model.parameters()`` will still consider these "
"layers."
msgstr ""

#: ../../../src/examples/llama-inference.rst:150
msgid "Generation"
msgstr ""

#: ../../../src/examples/llama-inference.rst:152
msgid ""
"Our ``Llama`` module can be used for training but not inference as the "
"``__call__`` method above processes one input, completely ignores the cache "
"and performs no sampling whatsoever. In the rest of this subsection, we will "
"implement the inference function as a python generator that processes the "
"prompt and then autoregressively yields tokens one at a time."
msgstr ""

#: ../../../src/examples/llama-inference.rst:158
msgid ""
"class Llama(nn.Module):\n"
"    ...\n"
"\n"
"    def generate(self, x, temp=1.0):\n"
"        cache = []\n"
"\n"
"        # Make an additive causal mask. We will need that to process the "
"prompt.\n"
"        mask = nn.MultiHeadAttention.create_additive_causal_mask(x."
"shape[1])\n"
"        mask = mask.astype(self.embedding.weight.dtype)\n"
"\n"
"        # First we process the prompt x the same way as in __call__ but\n"
"        # save the caches in cache\n"
"        x = self.embedding(x)\n"
"        for l in self.layers:\n"
"            x, c = l(x, mask=mask)\n"
"            cache.append(c)  # <--- we store the per layer cache in a\n"
"                             #      simple python list\n"
"        x = self.norm(x)\n"
"        y = self.out_proj(x[:, -1])  # <--- we only care about the last "
"logits\n"
"                                     #      that generate the next token\n"
"        y = mx.random.categorical(y * (1/temp))\n"
"\n"
"        # y now has size [1]\n"
"        # Since MLX is lazily evaluated nothing is computed yet.\n"
"        # Calling y.item() would force the computation to happen at\n"
"        # this point but we can also choose not to do that and let the\n"
"        # user choose when to start the computation.\n"
"        yield y\n"
"\n"
"        # Now we parsed the prompt and generated the first token we\n"
"        # need to feed it back into the model and loop to generate the\n"
"        # rest.\n"
"        while True:\n"
"            # Unsqueezing the last dimension to add a sequence length\n"
"            # dimension of 1\n"
"            x = y[:, None]\n"
"\n"
"            x = self.embedding(x)\n"
"            for i in range(len(cache)):\n"
"                # We are overwriting the arrays in the cache list. When\n"
"                # the computation will happen, MLX will be discarding the\n"
"                # old cache the moment it is not needed anymore.\n"
"                x, cache[i] = self.layers[i](x, mask=None, cache=cache[i])\n"
"            x = self.norm(x)\n"
"            y = self.out_proj(x[:, -1])\n"
"            y = mx.random.categorical(y * (1/temp))\n"
"\n"
"            yield y"
msgstr ""

#: ../../../src/examples/llama-inference.rst:210
msgid "Putting it all together"
msgstr ""

#: ../../../src/examples/llama-inference.rst:212
msgid ""
"We now have everything we need to create a Llama model and sample tokens "
"from it. In the following code, we randomly initialize a small Llama model, "
"process 6 tokens of prompt and generate 10 tokens."
msgstr ""

#: ../../../src/examples/llama-inference.rst:216
msgid ""
"model = Llama(num_layers=12, vocab_size=8192, dims=512, mlp_dims=1024, "
"num_heads=8)\n"
"\n"
"# Since MLX is lazily evaluated nothing has actually been materialized yet.\n"
"# We could have set the `dims` to 20_000 on a machine with 8GB of RAM and "
"the\n"
"# code above would still run. Let's actually materialize the model.\n"
"mx.eval(model.parameters())\n"
"\n"
"prompt = mx.array([[1, 10, 8, 32, 44, 7]])  # <-- Note the double brackets "
"because we\n"
"                                            #     have a batch dimension "
"even\n"
"                                            #     though it is 1 in this "
"case\n"
"\n"
"generated = [t for i, t in zip(range(10), model.generate(prompt, 0.8))]\n"
"\n"
"# Since we haven't evaluated anything, nothing is computed yet. The list\n"
"# `generated` contains the arrays that hold the computation graph for the\n"
"# full processing of the prompt and the generation of 10 tokens.\n"
"#\n"
"# We can evaluate them one at a time, or all together. Concatenate them or\n"
"# print them. They would all result in very similar runtimes and give "
"exactly\n"
"# the same results.\n"
"mx.eval(generated)"
msgstr ""

#: ../../../src/examples/llama-inference.rst:241
msgid "Converting the weights"
msgstr ""

#: ../../../src/examples/llama-inference.rst:243
msgid ""
"This section assumes that you have access to the original Llama weights and "
"the SentencePiece model that comes with them. We will write a small script "
"to convert the PyTorch weights to MLX compatible ones and write them in a "
"NPZ file that can be loaded directly by MLX."
msgstr ""

#: ../../../src/examples/llama-inference.rst:248
msgid ""
"import argparse\n"
"from itertools import starmap\n"
"\n"
"import numpy as np\n"
"import torch\n"
"\n"
"def map_torch_to_mlx(key, value):\n"
"    if \"tok_embedding\" in key:\n"
"        key = \"embedding.weight\"\n"
"\n"
"    elif \"norm\" in key:\n"
"        key = key.replace(\"attention_norm\", \"norm1\")."
"replace(\"ffn_norm\", \"norm2\")\n"
"\n"
"    elif \"wq\" in key or \"wk\" in key or \"wv\" in key or \"wo\" in key:\n"
"        key = key.replace(\"wq\", \"query_proj\")\n"
"        key = key.replace(\"wk\", \"key_proj\")\n"
"        key = key.replace(\"wv\", \"value_proj\")\n"
"        key = key.replace(\"wo\", \"out_proj\")\n"
"\n"
"    elif \"w1\" in key or \"w2\" in key or \"w3\" in key:\n"
"        # The FFN is a separate submodule in PyTorch\n"
"        key = key.replace(\"feed_forward.w1\", \"linear1\")\n"
"        key = key.replace(\"feed_forward.w3\", \"linear2\")\n"
"        key = key.replace(\"feed_forward.w2\", \"linear3\")\n"
"\n"
"    elif \"output\" in key:\n"
"        key = key.replace(\"output\", \"out_proj\")\n"
"\n"
"    elif \"rope\" in key:\n"
"        return None, None\n"
"\n"
"    return key, value.numpy()\n"
"\n"
"\n"
"if __name__ == \"__main__\":\n"
"    parser = argparse.ArgumentParser(description=\"Convert Llama weights to "
"MLX\")\n"
"    parser.add_argument(\"torch_weights\")\n"
"    parser.add_argument(\"output_file\")\n"
"    args = parser.parse_args()\n"
"\n"
"    state = torch.load(args.torch_weights)\n"
"    np.savez(\n"
"        args.output_file,\n"
"        **{k: v for k, v in starmap(map_torch_to_mlx, state.items()) if k is "
"not None}\n"
"    )"
msgstr ""

#: ../../../src/examples/llama-inference.rst:298
msgid "Weight loading and benchmarking"
msgstr ""

#: ../../../src/examples/llama-inference.rst:300
msgid ""
"After converting the weights to be compatible to our implementation, all "
"that is left is to load them from disk and we can finally use the LLM to "
"generate text. We can load numpy format files using the :func:`mlx.core."
"load` operation."
msgstr ""

#: ../../../src/examples/llama-inference.rst:304
msgid ""
"To create a parameter dictionary from the key/value representation of NPZ "
"files we will use the :func:`mlx.utils.tree_unflatten` helper method as "
"follows:"
msgstr ""

#: ../../../src/examples/llama-inference.rst:307
msgid ""
"from mlx.utils import tree_unflatten\n"
"\n"
"model.update(tree_unflatten(list(mx.load(weight_file).items())))"
msgstr ""

#: ../../../src/examples/llama-inference.rst:313
msgid ""
":meth:`mlx.utils.tree_unflatten` will take keys from the NPZ file that look "
"like ``layers.2.attention.query_proj.weight`` and will transform them to"
msgstr ""

#: ../../../src/examples/llama-inference.rst:316
msgid ""
"{\"layers\": [..., ..., {\"attention\": {\"query_proj\": "
"{\"weight\": ...}}}]}"
msgstr ""

#: ../../../src/examples/llama-inference.rst:320
msgid ""
"which can then be used to update the model. Note that the method above "
"incurs several unnecessary copies from disk to numpy and then from numpy to "
"MLX. It will be replaced in the future with direct loading to MLX."
msgstr ""

#: ../../../src/examples/llama-inference.rst:324
msgid ""
"You can download the full example code in `mlx-examples`_. Assuming, the "
"existence of ``weights.pth`` and ``tokenizer.model`` in the current working "
"directory we can play around with our inference script as follows (the "
"timings are representative of an M1 Ultra and the 7B parameter Llama model):"
msgstr ""

#: ../../../src/examples/llama-inference.rst:329
msgid ""
"$ python convert.py weights.pth llama-7B.mlx.npz\n"
"$ python llama.py llama-7B.mlx.npz tokenizer.model 'Call me Ishmael. Some "
"years ago never mind how long precisely'\n"
"[INFO] Loading model from disk: 5.247 s\n"
"Press enter to start generation\n"
"------\n"
", having little or no money in my purse, and nothing of greater consequence "
"in my mind, I happened to be walking down Gower Street in the afternoon, in "
"the heavy rain, and I saw a few steps off, a man in rags, who sat upon his "
"bundle and looked hard into the wet as if he were going to cry. I watched "
"him attentively for some time, and could not but observe that, though a "
"numerous crowd was hurrying up and down,\n"
"------\n"
"[INFO] Prompt processing: 0.437 s\n"
"[INFO] Full generation: 4.330 s"
msgstr ""

#: ../../../src/examples/llama-inference.rst:341
msgid ""
"We observe that 4.3 seconds are required to generate 100 tokens and 0.4 "
"seconds of those are spent processing the prompt. This amounts to a little "
"over **39 ms per token**."
msgstr ""

#: ../../../src/examples/llama-inference.rst:345
msgid ""
"By running with a much bigger prompt we can see that the per token "
"generation time as well as the prompt processing time remains almost "
"constant."
msgstr ""

#: ../../../src/examples/llama-inference.rst:348
msgid ""
"$ python llama.py llama-7B.mlx.npz tokenizer.model 'Call me Ishmael. Some "
"years ago never mind how long precisely, having little or no money in my "
"purse, and nothing of greater consequence in my mind, I happened to be "
"walking down Gower Street in the afternoon, in the heavy rain, and I saw a "
"few steps off, a man in rags, who sat upon his bundle and looked hard into "
"the wet as if he were going to cry. I watched him attentively for some time, "
"and could not but observe that, though a numerous crowd was hurrying up and "
"down, nobody took the least notice of him. I stopped at last, at a little "
"distance, as if I had been in doubt, and after looking on a few minutes, "
"walked straight up to him. He slowly raised his eyes, and fixed them upon me "
"for a moment, without speaking, and then resumed his place and posture as "
"before. I stood looking at him for a while, feeling very much pain at heart, "
"and then said to him, “What are you doing there?” Something like a smile "
"passed over his face, as he said slowly, “I am waiting for someone; but it "
"has been three quarters of an hour now, and he has not come.” “What is it "
"you are waiting for?” said I. Still he made no immediate reply, but again "
"put his face down upon his hands, and did not'\n"
"[INFO] Loading model from disk: 5.247 s\n"
"Press enter to start generation\n"
"------\n"
"take his eyes from the ground. “What is it you are waiting for?” said I. “I "
"am not accustomed to be thus questioned,” said he. “You look like a "
"reasonable man—tell me, then, what are you waiting for?” “You would not "
"understand,” he replied; “and how could you help me, if I were to tell you?” "
"“I should not only understand, but would do all that I could,” said I. He "
"did not\n"
"------\n"
"[INFO] Prompt processing: 0.579 s\n"
"[INFO] Full generation: 4.690 s\n"
"$ python llama.py --num-tokens 500 llama-7B.mlx.npz tokenizer.model 'Call me "
"Ishmael. Some years ago never mind how long precisely, having little or no "
"money in my purse, and nothing of greater consequence in my mind, I happened "
"to be walking down Gower Street in the afternoon, in the heavy rain, and I "
"saw a few steps off, a man in rags, who sat upon his bundle and looked hard "
"into the wet as if he were going to cry. I watched him attentively for some "
"time, and could not but observe that, though a numerous crowd was hurrying "
"up and down, nobody took the least notice of him. I stopped at last, at a "
"little distance, as if I had been in doubt, and after looking on a few "
"minutes, walked straight up to him. He slowly raised his eyes, and fixed "
"them upon me for a moment, without speaking, and then resumed his place and "
"posture as before. I stood looking at him for a while, feeling very much "
"pain at heart, and then said to him, “What are you doing there?” Something "
"like a smile passed over his face, as he said slowly, “I am waiting for "
"someone; but it has been three quarters of an hour now, and he has not "
"come.” “What is it you are waiting for?” said I. Still he made no immediate "
"reply, but again put his face down upon his hands, and did not'\n"
"[INFO] Loading model from disk: 5.628 s\n"
"Press enter to start generation\n"
"------\n"
"take his eyes from the ground. “What is it you are waiting for?” said I. “I "
"am not accustomed to be thus questioned,” said he. “You look like a "
"reasonable man—tell me, then, what are you waiting for?” “You would not "
"understand,” he replied; “and how could you help me, if I were to tell you?” "
"“I should not only understand, but would do all that I could,” said I. He "
"did not reply, but still went on looking at the ground, and took hold of his "
"bundle with a nervous trembling. I waited some time, and then resumed. “It "
"is of no use to say you would not understand, if I were to tell you,” said "
"he. “I have not told you why I am waiting for him,” said I. “And I am sure I "
"should not understand,” replied he. “I will tell you then,” said I, “and, "
"perhaps, you would not be surprised.” “No matter,” said he, “I shall be "
"surprised anyhow; so tell me why you are waiting for him.” “He is my "
"friend,” said I. “Yes,” said he, with a slight smile, “I know.” “He has been "
"kind to me,” said I, “and I am waiting for him. I want to see him, and could "
"have waited as I am now, for a much longer time.” “He will not soon come,” "
"said he. “Unless he sees you here, he will not know of your having waited, "
"and he will be very unlikely to come.” “No matter,” said I, “I shall wait "
"for him.” “This is a strange thing,” said he, still with the same amused "
"smile. “How did you know,” said I, “that he was coming? How should you be "
"waiting?” “That is my secret,” said he. “And you expect him?” “Yes,” said I. "
"“Are you disappointed then, if he does not come?” “No,” said I, “it is his "
"secret, not mine.” “If he comes,” said he, “do you mean to go straight "
"away?” “Yes,” said I, “I cannot be happy if I do not go straight away after "
"him.” “Did you know this place before?” asked he. “Yes,” said I. “Is there "
"any shop to buy food here?” “\n"
"------\n"
"[INFO] Prompt processing: 0.633 s\n"
"[INFO] Full generation: 21.475 s"
msgstr ""

#: ../../../src/examples/llama-inference.rst:368
msgid "Scripts"
msgstr ""

#: ../../../src/examples/llama-inference.rst:370
msgid "Download the code"
msgstr ""

#: ../../../src/examples/llama-inference.rst:372
msgid "The full example code is available in `mlx-examples`_."
msgstr ""

#: ../../../src/examples/llama-inference.rst:376
msgid ""
"Su, J., Lu, Y., Pan, S., Murtadha, A., Wen, B. and Liu, Y., 2021. Roformer: "
"Enhanced transformer with rotary position embedding. arXiv preprint "
"arXiv:2104.09864."
msgstr ""

#: ../../../src/examples/llama-inference.rst:379
msgid ""
"Zhang, B. and Sennrich, R., 2019. Root mean square layer normalization. "
"Advances in Neural Information Processing Systems, 32."
msgstr ""

#: ../../../src/examples/llama-inference.rst:381
msgid ""
"Shazeer, N., 2020. Glu variants improve transformer. arXiv preprint "
"arXiv:2002.05202."
msgstr ""
