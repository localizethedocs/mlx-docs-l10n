# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2023, Apple
# This file is distributed under the same license as the MLX package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: MLX 0.30\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-12-23 09:21+0000\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"Language: zh_CN\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"

#: ../../../src/python/optimizers/_autosummary/mlx.optimizers.Lion.rst:2
msgid "mlx.optimizers.Lion"
msgstr ""

#: ../../../../../../.conda/lib/python3.12/site-packages/mlx/optimizers/optimizers.py:docstring
#: of mlx.optimizers.optimizers.Lion:1
msgid "The Lion optimizer [1]."
msgstr ""

#: ../../../../../../.conda/lib/python3.12/site-packages/mlx/optimizers/optimizers.py:docstring
#: of mlx.optimizers.optimizers.Lion:3
msgid ""
"Since updates are computed through the sign operation, they tend to have "
"larger norm than for other optimizers such as SGD and Adam. We recommend a "
"learning rate that is 3-10x smaller than AdamW and a weight decay 3-10x "
"larger than AdamW to maintain the strength (lr * wd). Our Lion "
"implementation follows the original paper. In detail,"
msgstr ""

#: ../../../../../../.conda/lib/python3.12/site-packages/mlx/optimizers/optimizers.py:docstring
#: of mlx.optimizers.optimizers.Lion:10
msgid ""
"[1]: Chen, X. Symbolic Discovery of Optimization Algorithms. arXiv preprint "
"arXiv:2302.06675."
msgstr ""

#: ../../../../../../.conda/lib/python3.12/site-packages/mlx/optimizers/optimizers.py:docstring
#: of mlx.optimizers.optimizers.Lion:13
msgid ""
"c_{t + 1} &= \\beta_1 m_t + (1 - \\beta_1) g_t \\\\\n"
"m_{t + 1} &= \\beta_2 m_t + (1 - \\beta_2) g_t \\\\\n"
"w_{t + 1} &= w_t - \\eta (\\text{sign}(c_t) + \\lambda w_t)"
msgstr ""

#: ../../../src/python/optimizers/_autosummary/mlx.optimizers.Lion.rst:0
msgid "Parameters"
msgstr ""

#: ../../../../../../.conda/lib/python3.12/site-packages/mlx/optimizers/optimizers.py:docstring
#: of mlx.optimizers.optimizers.Lion:19
msgid "The learning rate :math:`\\eta`."
msgstr ""

#: ../../../../../../.conda/lib/python3.12/site-packages/mlx/optimizers/optimizers.py:docstring
#: of mlx.optimizers.optimizers.Lion:21
msgid ""
"The coefficients :math:`(\\beta_1, \\beta_2)` used for computing the "
"gradient momentum and update direction. Default: ``(0.9, 0.99)``"
msgstr ""

#: ../../../../../../.conda/lib/python3.12/site-packages/mlx/optimizers/optimizers.py:docstring
#: of mlx.optimizers.optimizers.Lion:25
msgid "The weight decay :math:`\\lambda`. Default: ``0.0``"
msgstr ""

#: ../../../src/python/optimizers/_autosummary/mlx.optimizers.Lion.rst:12
msgid "Methods"
msgstr ""

#: ../../../src/python/optimizers/_autosummary/mlx.optimizers.Lion.rst:17:<autosummary>:1
msgid ""
":py:obj:`__init__ <mlx.optimizers.Lion.__init__>`\\ \\(learning\\_rate\\[\\, "
"betas\\, weight\\_decay\\]\\)"
msgstr ""

#: ../../../src/python/optimizers/_autosummary/mlx.optimizers.Lion.rst:17:<autosummary>:1
msgid ""
":py:obj:`apply_single <mlx.optimizers.Lion.apply_single>`\\ \\(gradient\\, "
"parameter\\, state\\)"
msgstr ""

#: ../../../src/python/optimizers/_autosummary/mlx.optimizers.Lion.rst:17:<autosummary>:1
msgid ""
"Performs the Lion parameter update and stores :math:`m` in the optimizer "
"state."
msgstr ""

#: ../../../src/python/optimizers/_autosummary/mlx.optimizers.Lion.rst:17:<autosummary>:1
msgid ""
":py:obj:`init_single <mlx.optimizers.Lion.init_single>`\\ \\(parameter\\, "
"state\\)"
msgstr ""

#: ../../../src/python/optimizers/_autosummary/mlx.optimizers.Lion.rst:17:<autosummary>:1
msgid "Initialize optimizer state"
msgstr ""
