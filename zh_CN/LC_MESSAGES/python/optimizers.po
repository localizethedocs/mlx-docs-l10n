# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2023, MLX Contributors
# This file is distributed under the same license as the MLX package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: MLX 0.8\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-12-08 12:57+0000\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"Language: zh_CN\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"

#: ../../../src/python/optimizers.rst:4
msgid "Optimizers"
msgstr ""

#: ../../../src/python/optimizers.rst:6
msgid ""
"The optimizers in MLX can be used both with :mod:`mlx.nn` but also with "
"pure :mod:`mlx.core` functions. A typical example involves calling :meth:"
"`Optimizer.update` to update a model's parameters based on the loss "
"gradients and subsequently calling :func:`mlx.core.eval` to evaluate both "
"the model's parameters and the **optimizer state**."
msgstr ""

#: ../../../src/python/optimizers.rst:12
msgid ""
"# Create a model\n"
"model = MLP(num_layers, train_images.shape[-1], hidden_dim, num_classes)\n"
"mx.eval(model.parameters())\n"
"\n"
"# Create the gradient function and the optimizer\n"
"loss_and_grad_fn = nn.value_and_grad(model, loss_fn)\n"
"optimizer = optim.SGD(learning_rate=learning_rate)\n"
"\n"
"for e in range(num_epochs):\n"
"    for X, y in batch_iterate(batch_size, train_images, train_labels):\n"
"        loss, grads = loss_and_grad_fn(model, X, y)\n"
"\n"
"        # Update the model with the gradients. So far no computation has "
"happened.\n"
"        optimizer.update(model, grads)\n"
"\n"
"        # Compute the new parameters but also the optimizer state.\n"
"        mx.eval(model.parameters(), optimizer.state)"
msgstr ""
