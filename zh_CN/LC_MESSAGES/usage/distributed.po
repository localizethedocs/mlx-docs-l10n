# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2023, Apple
# This file is distributed under the same license as the MLX package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: MLX main\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-12-22 08:23+0000\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"Language: zh_CN\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"

#: ../../../src/usage/distributed.rst:4
msgid "Distributed Communication"
msgstr ""

#: ../../../src/usage/distributed.rst:8
msgid ""
"MLX supports distributed communication operations that allow the "
"computational cost of training or inference to be shared across many "
"physical machines. At the moment we support several different communication "
"backends introduced below."
msgstr ""

#: ../../../src/usage/distributed.rst:16
msgid "Backend"
msgstr ""

#: ../../../src/usage/distributed.rst:17
msgid "Description"
msgstr ""

#: ../../../src/usage/distributed.rst:18
msgid ":ref:`MPI <mpi_section>`"
msgstr ""

#: ../../../src/usage/distributed.rst:19
msgid "A full featured and mature distributed communications library."
msgstr ""

#: ../../../src/usage/distributed.rst:20
msgid ":ref:`RING <ring_section>`"
msgstr ""

#: ../../../src/usage/distributed.rst:21
msgid ""
"Ring all reduce and all gather over TCP sockets. Always available and "
"usually faster than MPI."
msgstr ""

#: ../../../src/usage/distributed.rst:23
msgid ":ref:`JACCL <jaccl_section>`"
msgstr ""

#: ../../../src/usage/distributed.rst:24
msgid ""
"Low latency communication with RDMA over thunderbolt. Necessary for things "
"like tensor parallelism."
msgstr ""

#: ../../../src/usage/distributed.rst:26
msgid ":ref:`NCCL <nccl_section>`"
msgstr ""

#: ../../../src/usage/distributed.rst:27
msgid "The backend of choice for CUDA environments."
msgstr ""

#: ../../../src/usage/distributed.rst:30
msgid ""
"The list of all currently supported operations and their documentation can "
"be seen in the :ref:`API docs<distributed>`."
msgstr ""

#: ../../../src/usage/distributed.rst:34
msgid "Getting Started"
msgstr ""

#: ../../../src/usage/distributed.rst:36
msgid "A distributed program in MLX is as simple as:"
msgstr ""

#: ../../../src/usage/distributed.rst:38
msgid ""
"import mlx.core as mx\n"
"\n"
"world = mx.distributed.init()\n"
"x = mx.distributed.all_sum(mx.ones(10))\n"
"print(world.rank(), x)"
msgstr ""

#: ../../../src/usage/distributed.rst:46
msgid ""
"The program above sums the array ``mx.ones(10)`` across all distributed "
"processes. However, when this script is run with ``python`` only one process "
"is launched and no distributed communication takes place. Namely, all "
"operations in ``mx.distributed`` are noops when the distributed group has a "
"size of one. This property allows us to avoid code that checks if we are in "
"a distributed setting similar to the one below:"
msgstr ""

#: ../../../src/usage/distributed.rst:53
msgid ""
"import mlx.core as mx\n"
"\n"
"x = ...\n"
"world = mx.distributed.init()\n"
"# No need for the check we can simply do x = mx.distributed.all_sum(x)\n"
"if world.size() > 1:\n"
"    x = mx.distributed.all_sum(x)"
msgstr ""

#: ../../../src/usage/distributed.rst:64
msgid "Running Distributed Programs"
msgstr ""

#: ../../../src/usage/distributed.rst:66
msgid ""
"MLX provides ``mlx.launch`` a helper script to launch distributed programs. "
"Continuing with our initial example we can run it on localhost with 4 "
"processes using"
msgstr ""

#: ../../../src/usage/distributed.rst:69
msgid ""
"$ mlx.launch -n 4 my_script.py\n"
"3 array([4, 4, 4, ..., 4, 4, 4], dtype=float32)\n"
"2 array([4, 4, 4, ..., 4, 4, 4], dtype=float32)\n"
"1 array([4, 4, 4, ..., 4, 4, 4], dtype=float32)\n"
"0 array([4, 4, 4, ..., 4, 4, 4], dtype=float32)"
msgstr ""

#: ../../../src/usage/distributed.rst:77
msgid ""
"We can also run it on some remote hosts by providing their IPs (provided "
"that the script exists on all hosts and they are reachable by ssh)"
msgstr ""

#: ../../../src/usage/distributed.rst:80
msgid ""
"$ mlx.launch --hosts ip1,ip2,ip3,ip4 my_script.py\n"
"3 array([4, 4, 4, ..., 4, 4, 4], dtype=float32)\n"
"2 array([4, 4, 4, ..., 4, 4, 4], dtype=float32)\n"
"1 array([4, 4, 4, ..., 4, 4, 4], dtype=float32)\n"
"0 array([4, 4, 4, ..., 4, 4, 4], dtype=float32)"
msgstr ""

#: ../../../src/usage/distributed.rst:88
msgid ""
"Consult the dedicated :doc:`usage guide<launching_distributed>` for more "
"information on using ``mlx.launch``."
msgstr ""

#: ../../../src/usage/distributed.rst:92
msgid "Selecting Backend"
msgstr ""

#: ../../../src/usage/distributed.rst:94
msgid ""
"You can select the backend you want to use when calling :func:`init` by "
"passing one of ``{'any', 'ring', 'jaccl', 'mpi', 'nccl'}``. When passing "
"``any``, MLX will try all available backends. If they all fail then a "
"singleton group is created."
msgstr ""

#: ../../../src/usage/distributed.rst:99
msgid ""
"After a distributed backend is successfully initialized :func:`init` will "
"return **the same backend** if called without arguments or with backend set "
"to ``any``."
msgstr ""

#: ../../../src/usage/distributed.rst:103
msgid ""
"The following examples aim to clarify the backend initialization logic in "
"MLX:"
msgstr ""

#: ../../../src/usage/distributed.rst:105
msgid ""
"# Case 1: Initialize MPI regardless if it was possible to initialize the "
"ring backend\n"
"world = mx.distributed.init(backend=\"mpi\")\n"
"world2 = mx.distributed.init()  # subsequent calls return the MPI backend!\n"
"\n"
"# Case 2: Initialize any backend\n"
"world = mx.distributed.init(backend=\"any\")  # equivalent to no arguments\n"
"world2 = mx.distributed.init()  # same as above\n"
"\n"
"# Case 3: Initialize both backends at the same time\n"
"world_mpi = mx.distributed.init(backend=\"mpi\")\n"
"world_ring = mx.distributed.init(backend=\"ring\")\n"
"world_any = mx.distributed.init()  # same as MPI because it was initialized "
"first!"
msgstr ""

#: ../../../src/usage/distributed.rst:123
msgid "Training Example"
msgstr ""

#: ../../../src/usage/distributed.rst:125
msgid ""
"In this section we will adapt an MLX training loop to support data parallel "
"distributed training. Namely, we will average the gradients across a set of "
"hosts before applying them to the model."
msgstr ""

#: ../../../src/usage/distributed.rst:129
msgid ""
"Our training loop looks like the following code snippet if we omit the "
"model, dataset and optimizer initialization."
msgstr ""

#: ../../../src/usage/distributed.rst:132
msgid ""
"model = ...\n"
"optimizer = ...\n"
"dataset = ...\n"
"\n"
"def step(model, x, y):\n"
"    loss, grads = loss_grad_fn(model, x, y)\n"
"    optimizer.update(model, grads)\n"
"    return loss\n"
"\n"
"for x, y in dataset:\n"
"    loss = step(model, x, y)\n"
"    mx.eval(loss, model.parameters())"
msgstr ""

#: ../../../src/usage/distributed.rst:147
msgid ""
"All we have to do to average the gradients across machines is perform an :"
"func:`all_sum` and divide by the size of the :class:`Group`. Namely we have "
"to :func:`mlx.utils.tree_map` the gradients with following function."
msgstr ""

#: ../../../src/usage/distributed.rst:151
msgid ""
"def all_avg(x):\n"
"    return mx.distributed.all_sum(x) / mx.distributed.init().size()"
msgstr ""

#: ../../../src/usage/distributed.rst:156
msgid ""
"Putting everything together our training loop step looks as follows with "
"everything else remaining the same."
msgstr ""

#: ../../../src/usage/distributed.rst:159
msgid ""
"from mlx.utils import tree_map\n"
"\n"
"def all_reduce_grads(grads):\n"
"    N = mx.distributed.init().size()\n"
"    if N == 1:\n"
"        return grads\n"
"    return tree_map(\n"
"        lambda x: mx.distributed.all_sum(x) / N,\n"
"        grads\n"
"    )\n"
"\n"
"def step(model, x, y):\n"
"    loss, grads = loss_grad_fn(model, x, y)\n"
"    grads = all_reduce_grads(grads)  # <--- This line was added\n"
"    optimizer.update(model, grads)\n"
"    return loss"
msgstr ""

#: ../../../src/usage/distributed.rst:179
msgid "Utilizing ``nn.average_gradients``"
msgstr ""

#: ../../../src/usage/distributed.rst:181
msgid ""
"Although the code example above works correctly; it performs one "
"communication per gradient. It is significantly more efficient to aggregate "
"several gradients together and perform fewer communication steps."
msgstr ""

#: ../../../src/usage/distributed.rst:185
msgid ""
"This is the purpose of :func:`mlx.nn.average_gradients`. The final code "
"looks almost identical to the example above:"
msgstr ""

#: ../../../src/usage/distributed.rst:188
msgid ""
"model = ...\n"
"optimizer = ...\n"
"dataset = ...\n"
"\n"
"def step(model, x, y):\n"
"    loss, grads = loss_grad_fn(model, x, y)\n"
"    grads = mx.nn.average_gradients(grads)  # <---- This line was added\n"
"    optimizer.update(model, grads)\n"
"    return loss\n"
"\n"
"for x, y in dataset:\n"
"    loss = step(model, x, y)\n"
"    mx.eval(loss, model.parameters())"
msgstr ""

#: ../../../src/usage/distributed.rst:207
msgid "Getting Started with Ring"
msgstr ""

#: ../../../src/usage/distributed.rst:209
msgid ""
"The ring backend does not depend on any third party library so it is always "
"available. It uses TCP sockets so the nodes need to be reachable via a "
"network. As the name suggests the nodes are connected in a ring which means "
"that rank 1 can only communicate with rank 0 and rank 2, rank 2 only with "
"rank 1 and rank 3 and so on and so forth. As a result :func:`send` and :func:"
"`recv` with arbitrary sender and receiver are not supported in the ring "
"backend."
msgstr ""

#: ../../../src/usage/distributed.rst:217
msgid "Defining a Ring"
msgstr ""

#: ../../../src/usage/distributed.rst:219
msgid ""
"The easiest way to define and use a ring is via a JSON hostfile and the "
"``mlx.launch`` :doc:`helper script <launching_distributed>`. For each node "
"one defines a hostname to ssh into to run commands on this node and one or "
"more IPs that this node will listen to for connections."
msgstr ""

#: ../../../src/usage/distributed.rst:224
msgid ""
"For example the hostfile below defines a 4 node ring. ``hostname1`` will be "
"rank 0, ``hostname2`` rank 1 etc."
msgstr ""

#: ../../../src/usage/distributed.rst:227
msgid ""
"[\n"
"    {\"ssh\": \"hostname1\", \"ips\": [\"123.123.123.1\"]},\n"
"    {\"ssh\": \"hostname2\", \"ips\": [\"123.123.123.2\"]},\n"
"    {\"ssh\": \"hostname3\", \"ips\": [\"123.123.123.3\"]},\n"
"    {\"ssh\": \"hostname4\", \"ips\": [\"123.123.123.4\"]}\n"
"]"
msgstr ""

#: ../../../src/usage/distributed.rst:236
msgid ""
"Running ``mlx.launch --hostfile ring-4.json my_script.py`` will ssh into "
"each node, run the script which will listen for connections in each of the "
"provided IPs. Specifically, ``hostname1`` will connect to ``123.123.123.2`` "
"and accept a connection from ``123.123.123.4`` and so on and so forth."
msgstr ""

#: ../../../src/usage/distributed.rst:242
msgid "Thunderbolt Ring"
msgstr ""

#: ../../../src/usage/distributed.rst:244
msgid ""
"Although the ring backend can have benefits over MPI even for Ethernet, its "
"main purpose is to use Thunderbolt rings for higher bandwidth communication. "
"Setting up such thunderbolt rings can be done manually, but is a relatively "
"tedious process. To simplify this, we provide the utility ``mlx."
"distributed_config``."
msgstr ""

#: ../../../src/usage/distributed.rst:249
msgid ""
"To use ``mlx.distributed_config`` your computers need to be accessible by "
"ssh via Ethernet or Wi-Fi. Subsequently, connect them via thunderbolt cables "
"and then call the utility as follows:"
msgstr ""

#: ../../../src/usage/distributed.rst:253
msgid ""
"mlx.distributed_config --verbose --hosts host1,host2,host3,host4 --backend "
"ring"
msgstr ""

#: ../../../src/usage/distributed.rst:257
msgid ""
"By default the script will attempt to discover the thunderbolt ring and "
"provide you with the commands to configure each node as well as the "
"``hostfile.json`` to use with ``mlx.launch``. If password-less ``sudo`` is "
"available on the nodes then ``--auto-setup`` can be used to configure them "
"automatically."
msgstr ""

#: ../../../src/usage/distributed.rst:262
msgid ""
"If you want to go through the process manually, the steps are as follows:"
msgstr ""

#: ../../../src/usage/distributed.rst:264
msgid "Disable the thunderbolt bridge interface"
msgstr ""

#: ../../../src/usage/distributed.rst:265
msgid ""
"For the cable connecting rank ``i`` to rank ``i + 1`` find the interfaces "
"corresponding to that cable in nodes ``i`` and ``i + 1``."
msgstr ""

#: ../../../src/usage/distributed.rst:267
msgid ""
"Set up a unique subnetwork connecting the two nodes for the corresponding "
"interfaces. For instance if the cable corresponds to ``en2`` on node ``i`` "
"and ``en2`` also on node ``i + 1`` then we may assign IPs ``192.168.0.1`` "
"and ``192.168.0.2`` respectively to the two nodes. For more details you can "
"see the commands prepared by the utility script."
msgstr ""

#: ../../../src/usage/distributed.rst:276
msgid "Getting Started with JACCL"
msgstr ""

#: ../../../src/usage/distributed.rst:278
msgid ""
"Starting from macOS 26.2, RDMA over thunderbolt is available and enables low-"
"latency communication between Macs with thunderbolt 5. MLX provides the "
"JACCL backend that uses this functionality to achieve communication latency "
"an order of magnitude lower than the ring backend."
msgstr ""

#: ../../../src/usage/distributed.rst:285
msgid ""
"The name JACCL (pronounced Jackal) stands for *Jack and Angelos' Collective "
"Communication Library* and it is an obvious pun to Nvidia's NCCL but also "
"tribute to *Jack Beasley* who led the development of RDMA over Thunderbolt "
"at Apple."
msgstr ""

#: ../../../src/usage/distributed.rst:291
msgid "Enabling RDMA"
msgstr ""

#: ../../../src/usage/distributed.rst:293
msgid ""
"Until the feature matures, enabling RDMA over thunderbolt is slightly more "
"involved and **cannot** be done remotely even with sudo. In fact, it has to "
"be done in macOS recovery:"
msgstr ""

#: ../../../src/usage/distributed.rst:297
msgid ""
"`Start your computer in recovery <https://support.apple.com/en-us/102518>`_."
msgstr ""

#: ../../../src/usage/distributed.rst:298
msgid "Open the Terminal by going to Utilities -> Terminal."
msgstr ""

#: ../../../src/usage/distributed.rst:299
msgid "Run ``rdma_ctl enable``."
msgstr ""

#: ../../../src/usage/distributed.rst:300
msgid "Reboot."
msgstr ""

#: ../../../src/usage/distributed.rst:302
msgid ""
"To verify that you have successfully enabled Thunderbolt RDMA you can run "
"``ibv_devices`` which should produce something like the following for an M3 "
"Ultra."
msgstr ""

#: ../../../src/usage/distributed.rst:305
msgid ""
"~ % ibv_devices\n"
"device                 node GUID\n"
"------              ----------------\n"
"rdma_en2            8096a9d9edbaac05\n"
"rdma_en3            8196a9d9edbaac05\n"
"rdma_en5            8396a9d9edbaac05\n"
"rdma_en4            8296a9d9edbaac05\n"
"rdma_en6            8496a9d9edbaac05\n"
"rdma_en7            8596a9d9edbaac05"
msgstr ""

#: ../../../src/usage/distributed.rst:318
msgid "Defining a Mesh"
msgstr ""

#: ../../../src/usage/distributed.rst:320
msgid ""
"The JACCL backend supports only fully connected topologies. Namely, there "
"needs to be a thunderbolt cable connecting all pairs of Macs directly. For "
"example, in the following topology visualizations, the left one is valid "
"because there is a connection from any node to any other node, while for the "
"one on the right M3 Ultra 1 is not connected to M3 Ultra 2."
msgstr ""

#: ../../../src/usage/distributed.rst:326
msgid ""
"<div style=\"display: flex; text-align: center; align-items: end; font-size: "
"80%;\">\n"
"  <div>\n"
"    <img src=\"../_static/distributed/m3-ultra-mesh.png\" alt=\"M3 Ultra "
"thunderbolt mesh\" style=\"width: 55%\">\n"
"    <p>Fully connected mesh of four M3 Ultra.</p>\n"
"  </div>\n"
"  <div>\n"
"    <img src=\"../_static/distributed/m3-ultra-mesh-broken.png\" alt=\"M3 "
"Ultra broken thunderbolt mesh\" style=\"width: 55%\">\n"
"    <p>Not a valid mesh (M3 Ultra 1 is not connected to M3 Ultra 2).</p>\n"
"  </div>\n"
"</div>"
msgstr ""

#: ../../../src/usage/distributed.rst:339
msgid ""
"Similar to the ring backend, the easiest way to use JACCL with MLX is to "
"write a JSON hostfile that will be used by ``mlx.launch``. The hostfile "
"needs to contain"
msgstr ""

#: ../../../src/usage/distributed.rst:342
msgid "Hostnames to use for launching scripts via ssh"
msgstr ""

#: ../../../src/usage/distributed.rst:343
msgid "An IP for rank 0 that is reachable by all nodes"
msgstr ""

#: ../../../src/usage/distributed.rst:344
msgid "A list of rdma devices that connect each node to each other node"
msgstr ""

#: ../../../src/usage/distributed.rst:346
msgid "The following JSON defines the valid 4-node mesh from the image above."
msgstr ""

#: ../../../src/usage/distributed.rst:348
msgid ""
"[\n"
"    {\n"
"        \"ssh\": \"m3-ultra-1\",\n"
"        \"ips\": [\"123.123.123.1\"],\n"
"        \"rdma\": [null, \"rdma_en5\", \"rdma_en4\", \"rdma_en3\"]\n"
"    },\n"
"    {\n"
"        \"ssh\": \"m3-ultra-2\",\n"
"        \"ips\": [],\n"
"        \"rdma\": [\"rdma_en5\", null, \"rdma_en3\", \"rdma_en4\"]\n"
"    },\n"
"    {\n"
"        \"ssh\": \"m3-ultra-3\",\n"
"        \"ips\": [],\n"
"        \"rdma\": [\"rdma_en4\", \"rdma_en3\", null, \"rdma_en5\"]\n"
"    },\n"
"    {\n"
"        \"ssh\": \"m3-ultra-4\",\n"
"        \"ips\": [],\n"
"        \"rdma\": [\"rdma_en3\", \"rdma_en4\", \"rdma_en5\", null]\n"
"    }\n"
"]"
msgstr ""

#: ../../../src/usage/distributed.rst:373
msgid ""
"Even though TCP/IP is not used when communicating with Thunderbolt RDMA, "
"disabling the thunderbolt bridge is still required as well as setting up "
"isolated local networks for each thunderbolt connection."
msgstr ""

#: ../../../src/usage/distributed.rst:377
msgid ""
"All of the above can be done instead via ``mlx.distributed_config``. This "
"helper script will"
msgstr ""

#: ../../../src/usage/distributed.rst:380
msgid "ssh into each node"
msgstr ""

#: ../../../src/usage/distributed.rst:381
msgid "extract the thunderbolt connectivity"
msgstr ""

#: ../../../src/usage/distributed.rst:382
msgid "check for a valid mesh"
msgstr ""

#: ../../../src/usage/distributed.rst:383
msgid ""
"provide the commands to configure each node (or run them if sudo is "
"available)"
msgstr ""

#: ../../../src/usage/distributed.rst:384
msgid "generate the hostfile to be used with ``mlx.launch``"
msgstr ""

#: ../../../src/usage/distributed.rst:387
msgid "Putting It All Together"
msgstr ""

#: ../../../src/usage/distributed.rst:389
msgid ""
"For example launching a distributed MLX script that uses JACCL is fairly "
"simple if the nodes are reachable via ssh and have password-less sudo."
msgstr ""

#: ../../../src/usage/distributed.rst:392
msgid ""
"First, connect all the thunderbolt cables. Then we can verify the "
"connections by using the ``mlx.distributed_config`` script to visualize them."
msgstr ""

#: ../../../src/usage/distributed.rst:395
msgid ""
"mlx.distributed_config --verbose \\\n"
"     --hosts m3-ultra-1,m3-ultra-2,m3-ultra-3,m3-ultra-4 \\\n"
"     --over thunderbolt --dot | dot -Tpng | open -f -a Preview"
msgstr ""

#: ../../../src/usage/distributed.rst:401
msgid ""
"After making sure that everything looks right we can auto-configure the "
"nodes and save the hostfile to ``m3-ultra-jaccl.json`` by running:"
msgstr ""

#: ../../../src/usage/distributed.rst:404
msgid ""
"mlx.distributed_config --verbose \\\n"
"     --hosts m3-ultra-1,m3-ultra-2,m3-ultra-3,m3-ultra-4 \\\n"
"     --over thunderbolt --backend jaccl \\\n"
"     --auto-setup --output m3-ultra-jaccl.json"
msgstr ""

#: ../../../src/usage/distributed.rst:411
msgid ""
"And now we are ready to run a distributed MLX script such as distributed "
"inference of a gigantic model using MLX LM."
msgstr ""

#: ../../../src/usage/distributed.rst:414
msgid ""
"mlx.launch --verbose --backend jaccl --hostfile m3-ultra-jaccl.json \\\n"
"     --env MLX_METAL_FAST_SYNCH=1 -- \\  # <--- important\n"
"     /path/to/remote/python -m mlx_lm chat --model mlx-community/DeepSeek-"
"R1-0528-4bit"
msgstr ""

#: ../../../src/usage/distributed.rst:422
msgid ""
"Defining the environment variable ``MLX_METAL_FAST_SYNCH=1`` enables a "
"different, faster way of synchronizing between the GPU and the CPU. It is "
"not specific to the JACCL backend and can be used in all cases where the CPU "
"and GPU need to collaborate for some computation and is pretty critical for "
"low-latency communication since the communication is done by the CPU."
msgstr ""

#: ../../../src/usage/distributed.rst:431
msgid "Getting Started with NCCL"
msgstr ""

#: ../../../src/usage/distributed.rst:433
msgid ""
"MLX on CUDA environments ships with the ability to talk to `NCCL <https://"
"developer.nvidia.com/nccl>`_ which is a high-performance collective "
"communication library that supports both multi-gpu and multi-node setups."
msgstr ""

#: ../../../src/usage/distributed.rst:437
msgid ""
"For CUDA environments, NCCL is the default backend for ``mlx.launch`` and "
"all it takes to run a distributed job is"
msgstr ""

#: ../../../src/usage/distributed.rst:440
msgid ""
"mlx.launch -n 8 test.py\n"
"\n"
"# perfect for interactive scripts\n"
"mlx.launch -n 8 python -m mlx_lm chat --model my-model"
msgstr ""

#: ../../../src/usage/distributed.rst:447
msgid ""
"You can also use ``mlx.launch`` to ssh to a remote node and launch a script "
"with the same ease"
msgstr ""

#: ../../../src/usage/distributed.rst:450
msgid "mlx.launch --hosts my-cuda-node -n 8 test.py"
msgstr ""

#: ../../../src/usage/distributed.rst:454
msgid ""
"In many cases you may not want to use ``mlx.launch`` with the NCCL backend "
"because the cluster scheduler will be the one launching the processes. You "
"can :ref:`see which environment variables need to be defined "
"<no_mlx_launch>` in order for the MLX NCCL backend to be initialized "
"correctly."
msgstr ""

#: ../../../src/usage/distributed.rst:462
msgid "Getting Started with MPI"
msgstr ""

#: ../../../src/usage/distributed.rst:464
msgid ""
"MLX already comes with the ability to \"talk\" to `MPI <https://en.wikipedia."
"org/wiki/Message_Passing_Interface>`_ if it is installed on the machine. "
"Launching distributed MLX programs that use MPI can be done with ``mpirun`` "
"as expected. However, in the following examples we will be using ``mlx."
"launch --backend mpi`` which takes care of some nuisances such as setting "
"absolute paths for the ``mpirun`` executable and the ``libmpi.dyld`` shared "
"library."
msgstr ""

#: ../../../src/usage/distributed.rst:472
msgid ""
"The simplest possible usage is the following which, assuming the minimal "
"example in the beginning of this page, should result in:"
msgstr ""

#: ../../../src/usage/distributed.rst:475
msgid ""
"$ mlx.launch --backend mpi -n 2 test.py\n"
"1 array([2, 2, 2, ..., 2, 2, 2], dtype=float32)\n"
"0 array([2, 2, 2, ..., 2, 2, 2], dtype=float32)"
msgstr ""

#: ../../../src/usage/distributed.rst:481
msgid ""
"The above launches two processes on the same (local) machine and we can see "
"both standard output streams. The processes send the array of 1s to each "
"other and compute the sum which is printed. Launching with ``mlx.launch -n "
"4 ...`` would print 4 etc."
msgstr ""

#: ../../../src/usage/distributed.rst:487
msgid "Installing MPI"
msgstr ""

#: ../../../src/usage/distributed.rst:489
msgid ""
"MPI can be installed with Homebrew, pip, using the Anaconda package manager, "
"or compiled from source. Most of our testing is done using ``openmpi`` "
"installed with the Anaconda package manager as follows:"
msgstr ""

#: ../../../src/usage/distributed.rst:493
msgid "$ conda install conda-forge::openmpi"
msgstr ""

#: ../../../src/usage/distributed.rst:497
msgid ""
"Installing with Homebrew or pip requires specifying the location of ``libmpi."
"dyld`` so that MLX can find it and load it at runtime. This can simply be "
"achieved by passing the ``DYLD_LIBRARY_PATH`` environment variable to "
"``mpirun`` and it is done automatically by ``mlx.launch``. Some environments "
"use a non-standard library filename that can be specified using the "
"``MPI_LIBNAME`` environment variable. This is automatically taken care of by "
"``mlx.launch`` as well."
msgstr ""

#: ../../../src/usage/distributed.rst:504
msgid ""
"$ mpirun -np 2 -x DYLD_LIBRARY_PATH=/opt/homebrew/lib/ -x "
"MPI_LIBNAME=libmpi.40.dylib python test.py\n"
"$ # or simply\n"
"$ mlx.launch -n 2 test.py"
msgstr ""

#: ../../../src/usage/distributed.rst:511
msgid "Setting up Remote Hosts"
msgstr ""

#: ../../../src/usage/distributed.rst:513
msgid ""
"MPI can automatically connect to remote hosts and set up the communication "
"over the network if the remote hosts can be accessed via ssh. A good "
"checklist to debug connectivity issues is the following:"
msgstr ""

#: ../../../src/usage/distributed.rst:517
msgid ""
"``ssh hostname`` works from all machines to all machines without asking for "
"password or host confirmation"
msgstr ""

#: ../../../src/usage/distributed.rst:519
msgid "``mpirun`` is accessible on all machines."
msgstr ""

#: ../../../src/usage/distributed.rst:520
msgid ""
"Ensure that the ``hostname`` used by MPI is the one that you have configured "
"in the ``.ssh/config`` files on all machines."
msgstr ""

#: ../../../src/usage/distributed.rst:524
msgid "Tuning MPI All Reduce"
msgstr ""

#: ../../../src/usage/distributed.rst:528
msgid ""
"For faster all reduce consider using the ring backend either with "
"Thunderbolt connections or over Ethernet."
msgstr ""

#: ../../../src/usage/distributed.rst:531
msgid ""
"Configure MPI to use N tcp connections between each host to improve "
"bandwidth by passing ``--mca btl_tcp_links N``."
msgstr ""

#: ../../../src/usage/distributed.rst:534
msgid ""
"Force MPI to use the most performant network interface by setting ``--mca "
"btl_tcp_if_include <iface>`` where ``<iface>`` should be the interface you "
"want to use."
msgstr ""

#: ../../../src/usage/distributed.rst:541
msgid "Distributed Without ``mlx.launch``"
msgstr ""

#: ../../../src/usage/distributed.rst:543
msgid ""
"None of the implementations of the distributed backends require launching "
"with ``mlx.launch``. The script simply connects to each host. Starts a "
"process per rank and sets up the necessary environment variables before "
"delegating to your MLX script. See the :doc:`dedicated documentation page "
"<launching_distributed>` for more details."
msgstr ""

#: ../../../src/usage/distributed.rst:549
msgid ""
"For many use-cases this will be the easiest way to perform distributed "
"computations in MLX. However, there may be reasons that you cannot or should "
"not use ``mlx.launch``. A common such case is the use of a scheduler that "
"starts all the processes for you on machines undetermined at the time of "
"scheduling the job."
msgstr ""

#: ../../../src/usage/distributed.rst:555
msgid "Below we list the environment variables required to use each backend."
msgstr ""

#: ../../../src/usage/distributed.rst:558
msgid "Ring"
msgstr ""

#: ../../../src/usage/distributed.rst:560
#: ../../../src/usage/distributed.rst:581
#: ../../../src/usage/distributed.rst:604
msgid ""
"**MLX_RANK** should contain a single 0-based integer that defines the rank "
"of the process."
msgstr ""

#: ../../../src/usage/distributed.rst:563
msgid ""
"**MLX_HOSTFILE** should contain the path to a json file that contains IPs "
"and ports for each rank to listen to, something like the following:"
msgstr ""

#: ../../../src/usage/distributed.rst:566
msgid ""
"[\n"
"  [\"123.123.1.1:5000\", \"123.123.1.2:5000\"],\n"
"  [\"123.123.2.1:5000\", \"123.123.2.2:5000\"],\n"
"  [\"123.123.3.1:5000\", \"123.123.3.2:5000\"],\n"
"  [\"123.123.4.1:5000\", \"123.123.4.2:5000\"]\n"
"]"
msgstr ""

#: ../../../src/usage/distributed.rst:575
msgid ""
"**MLX_RING_VERBOSE** is optional and if set to 1 it enables some more "
"logging from the distributed backend."
msgstr ""

#: ../../../src/usage/distributed.rst:579
msgid "JACCL"
msgstr ""

#: ../../../src/usage/distributed.rst:584
msgid ""
"**MLX_JACCL_COORDINATOR** should contain the IP and port that rank 0 can "
"listen to all the other ranks connect to in order to establish the RDMA "
"connections."
msgstr ""

#: ../../../src/usage/distributed.rst:587
msgid ""
"**MLX_IBV_DEVICES** should contain the path to a json file that contains the "
"ibverbs device names that connect each node to each other node, something "
"like the following:"
msgstr ""

#: ../../../src/usage/distributed.rst:591
msgid ""
"[\n"
"   [null, \"rdma_en5\", \"rdma_en4\", \"rdma_en3\"],\n"
"   [\"rdma_en5\", null, \"rdma_en3\", \"rdma_en4\"],\n"
"   [\"rdma_en4\", \"rdma_en3\", null, \"rdma_en5\"],\n"
"   [\"rdma_en3\", \"rdma_en4\", \"rdma_en5\", null]\n"
"]"
msgstr ""

#: ../../../src/usage/distributed.rst:602
msgid "NCCL"
msgstr ""

#: ../../../src/usage/distributed.rst:607
msgid ""
"**MLX_WORLD_SIZE** should contain the total number of processes that will be "
"launched."
msgstr ""

#: ../../../src/usage/distributed.rst:610
msgid ""
"**NCCL_HOST_IP** and **NCCL_PORT** should contain the IP and port that all "
"hosts can connect to to establish the NCCL communication."
msgstr ""

#: ../../../src/usage/distributed.rst:613
msgid ""
"**CUDA_VISIBLE_DEVICES** should contain the local index of the gpu that "
"corresponds to this process."
msgstr ""

#: ../../../src/usage/distributed.rst:616
msgid ""
"Of course any `other environment variable <https://docs.nvidia.com/"
"deeplearning/nccl/user-guide/docs/env.html>`_ that is used by NCCL can be "
"set."
msgstr ""

#: ../../../src/usage/distributed.rst:623
msgid "Tips and Tricks"
msgstr ""

#: ../../../src/usage/distributed.rst:625
msgid ""
"This is a small collection of tips to help you utilize better the "
"distributed communication capabilities of MLX."
msgstr ""

#: ../../../src/usage/distributed.rst:628
msgid "*Test locally first.*"
msgstr ""

#: ../../../src/usage/distributed.rst:630
msgid ""
"You can use the pattern ``mlx.launch -n2 -- my_script.py`` to run a small "
"scale test on a single node first."
msgstr ""

#: ../../../src/usage/distributed.rst:633
msgid "*Batch your communication.*"
msgstr ""

#: ../../../src/usage/distributed.rst:635
msgid ""
"As described in the :ref:`training example <training_example>`, performing a "
"lot of small communications can hurt performance. Copy the approach of :func:"
"`mlx.nn.average_gradients` to gather many small communications in a single "
"large one."
msgstr ""

#: ../../../src/usage/distributed.rst:640
msgid "*Visualize the connectivity.*"
msgstr ""

#: ../../../src/usage/distributed.rst:642
msgid ""
"Use ``mlx.distributed_config --hosts h1,h2,h3 --over thunderbolt --dot`` to "
"visualize the connnections and make sure that the cables are connected "
"correctly. See the :ref:`JACCL section <jaccl_section>` for examples."
msgstr ""

#: ../../../src/usage/distributed.rst:646
msgid "*Use the debugger.*"
msgstr ""

#: ../../../src/usage/distributed.rst:648
msgid ""
"``mlx.launch`` is meant for interactive use. It broadcasts stdin to all "
"processes and gathers stdout from all processes. This makes using ``pdb`` a "
"breeze."
msgstr ""
