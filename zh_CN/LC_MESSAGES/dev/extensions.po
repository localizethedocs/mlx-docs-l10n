# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2023, MLX Contributors
# This file is distributed under the same license as the MLX package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: MLX 0.17\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-12-08 12:57+0000\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"Language: zh_CN\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"

#: ../../../src/dev/extensions.rst:2
msgid "Custom Extensions in MLX"
msgstr ""

#: ../../../src/dev/extensions.rst:4
msgid ""
"You can extend MLX with custom operations on the CPU or GPU. This guide "
"explains how to do that with a simple example."
msgstr ""

#: ../../../src/dev/extensions.rst:8
msgid "Introducing the Example"
msgstr ""

#: ../../../src/dev/extensions.rst:10
msgid ""
"Let's say you would like an operation that takes in two arrays, ``x`` and "
"``y``, scales them both by coefficients ``alpha`` and ``beta`` respectively, "
"and then adds them together to get the result ``z = alpha * x + beta * y``. "
"You can do that in MLX directly:"
msgstr ""

#: ../../../src/dev/extensions.rst:15
msgid ""
"import mlx.core as mx\n"
"\n"
"def simple_axpby(x: mx.array, y: mx.array, alpha: float, beta: float) -> mx."
"array:\n"
"    return alpha * x + beta * y"
msgstr ""

#: ../../../src/dev/extensions.rst:22
msgid ""
"This function performs that operation while leaving the implementation and "
"function transformations to MLX."
msgstr ""

#: ../../../src/dev/extensions.rst:25
msgid ""
"However you may need to customize the underlying implementation, perhaps to "
"make it faster or for custom differentiation. In this tutorial we will go "
"through adding custom extensions. It will cover:"
msgstr ""

#: ../../../src/dev/extensions.rst:29
msgid "The structure of the MLX library."
msgstr ""

#: ../../../src/dev/extensions.rst:30
msgid ""
"Implementing a CPU operation that redirects to Accelerate_ when appropriate."
msgstr ""

#: ../../../src/dev/extensions.rst:31
msgid "Implementing a GPU operation using metal."
msgstr ""

#: ../../../src/dev/extensions.rst:32
msgid "Adding the ``vjp`` and ``jvp`` function transformation."
msgstr ""

#: ../../../src/dev/extensions.rst:33
msgid "Building a custom extension and binding it to python."
msgstr ""

#: ../../../src/dev/extensions.rst:36
msgid "Operations and Primitives"
msgstr ""

#: ../../../src/dev/extensions.rst:38
msgid ""
"Operations in MLX build the computation graph. Primitives provide the rules "
"for evaluating and transforming the graph. Let's start by discussing "
"operations in more detail."
msgstr ""

#: ../../../src/dev/extensions.rst:43
msgid "Operations"
msgstr ""

#: ../../../src/dev/extensions.rst:45
msgid ""
"Operations are the front-end functions that operate on arrays. They are "
"defined in the C++ API (:ref:`cpp_ops`), and the Python API (:ref:`ops`) "
"binds them."
msgstr ""

#: ../../../src/dev/extensions.rst:48
msgid ""
"We would like an operation, :meth:`axpby` that takes in two arrays ``x`` and "
"``y``, and two scalars, ``alpha`` and ``beta``. This is how to define it in "
"C++:"
msgstr ""

#: ../../../src/dev/extensions.rst:52
msgid ""
"/**\n"
"*  Scale and sum two vectors element-wise\n"
"*  z = alpha * x + beta * y\n"
"*\n"
"*  Follow numpy style broadcasting between x and y\n"
"*  Inputs are upcasted to floats if needed\n"
"**/\n"
"array axpby(\n"
"    const array& x, // Input array x\n"
"    const array& y, // Input array y\n"
"    const float alpha, // Scaling factor for x\n"
"    const float beta, // Scaling factor for y\n"
"    StreamOrDevice s = {} // Stream on which to schedule the operation\n"
");"
msgstr ""

#: ../../../src/dev/extensions.rst:69
msgid "The simplest way to this operation is in terms of existing operations:"
msgstr ""

#: ../../../src/dev/extensions.rst:71
msgid ""
"array axpby(\n"
"    const array& x, // Input array x\n"
"    const array& y, // Input array y\n"
"    const float alpha, // Scaling factor for x\n"
"    const float beta, // Scaling factor for y\n"
"    StreamOrDevice s /* = {} */ // Stream on which to schedule the "
"operation\n"
") {\n"
"    // Scale x and y on the provided stream\n"
"    auto ax = multiply(array(alpha), x, s);\n"
"    auto by = multiply(array(beta), y, s);\n"
"\n"
"    // Add and return\n"
"    return add(ax, by, s);\n"
"}"
msgstr ""

#: ../../../src/dev/extensions.rst:88
msgid ""
"The operations themselves do not contain the implementations that act on the "
"data, nor do they contain the rules of transformations. Rather, they are an "
"easy to use interface that use :class:`Primitive` building blocks."
msgstr ""

#: ../../../src/dev/extensions.rst:93
msgid "Primitives"
msgstr ""

#: ../../../src/dev/extensions.rst:95
msgid ""
"A :class:`Primitive` is part of the computation graph of an :class:`array`. "
"It defines how to create outputs arrays given a input arrays. Further, a :"
"class:`Primitive` has methods to run on the CPU or GPU and for function "
"transformations such as ``vjp`` and ``jvp``.  Lets go back to our example to "
"be more concrete:"
msgstr ""

#: ../../../src/dev/extensions.rst:101
msgid ""
"class Axpby : public Primitive {\n"
"  public:\n"
"    explicit Axpby(Stream stream, float alpha, float beta)\n"
"        : Primitive(stream), alpha_(alpha), beta_(beta){};\n"
"\n"
"    /**\n"
"    * A primitive must know how to evaluate itself on the CPU/GPU\n"
"    * for the given inputs and populate the output array.\n"
"    *\n"
"    * To avoid unnecessary allocations, the evaluation function\n"
"    * is responsible for allocating space for the array.\n"
"    */\n"
"    void eval_cpu(\n"
"        const std::vector<array>& inputs,\n"
"        std::vector<array>& outputs) override;\n"
"    void eval_gpu(\n"
"        const std::vector<array>& inputs,\n"
"        std::vector<array>& outputs) override;\n"
"\n"
"    /** The Jacobian-vector product. */\n"
"    std::vector<array> jvp(\n"
"        const std::vector<array>& primals,\n"
"        const std::vector<array>& tangents,\n"
"        const std::vector<int>& argnums) override;\n"
"\n"
"    /** The vector-Jacobian product. */\n"
"    std::vector<array> vjp(\n"
"        const std::vector<array>& primals,\n"
"        const array& cotan,\n"
"        const std::vector<int>& argnums,\n"
"        const std::vector<array>& outputs) override;\n"
"\n"
"    /**\n"
"    * The primitive must know how to vectorize itself across\n"
"    * the given axes. The output is a pair containing the array\n"
"    * representing the vectorized computation and the axis which\n"
"    * corresponds to the output vectorized dimension.\n"
"    */\n"
"    virtual std::pair<std::vector<array>, std::vector<int>> vmap(\n"
"        const std::vector<array>& inputs,\n"
"        const std::vector<int>& axes) override;\n"
"\n"
"    /** Print the primitive. */\n"
"    void print(std::ostream& os) override {\n"
"        os << \"Axpby\";\n"
"    }\n"
"\n"
"    /** Equivalence check **/\n"
"    bool is_equivalent(const Primitive& other) const override;\n"
"\n"
"  private:\n"
"    float alpha_;\n"
"    float beta_;\n"
"\n"
"    /** Fall back implementation for evaluation on CPU */\n"
"    void eval(const std::vector<array>& inputs, array& out);\n"
"};"
msgstr ""

#: ../../../src/dev/extensions.rst:161
msgid ""
"The :class:`Axpby` class derives from the base :class:`Primitive` class. "
"The :class:`Axpby` treats ``alpha`` and ``beta`` as parameters. It then "
"provides implementations of how the output array is produced given the "
"inputs through :meth:`Axpby::eval_cpu` and :meth:`Axpby::eval_gpu`. It also "
"provides rules of transformations in :meth:`Axpby::jvp`, :meth:`Axpby::vjp`, "
"and :meth:`Axpby::vmap`."
msgstr ""

#: ../../../src/dev/extensions.rst:169
msgid "Using the Primitive"
msgstr ""

#: ../../../src/dev/extensions.rst:171
msgid ""
"Operations can use this :class:`Primitive` to add a new :class:`array` to "
"the computation graph. An :class:`array` can be constructed by providing its "
"data type, shape, the :class:`Primitive` that computes it, and the :class:"
"`array` inputs that are passed to the primitive."
msgstr ""

#: ../../../src/dev/extensions.rst:176
msgid ""
"Let's reimplement our operation now in terms of our :class:`Axpby` primitive."
msgstr ""

#: ../../../src/dev/extensions.rst:178
msgid ""
"array axpby(\n"
"    const array& x, // Input array x\n"
"    const array& y, // Input array y\n"
"    const float alpha, // Scaling factor for x\n"
"    const float beta, // Scaling factor for y\n"
"    StreamOrDevice s /* = {} */ // Stream on which to schedule the "
"operation\n"
") {\n"
"    // Promote dtypes between x and y as needed\n"
"    auto promoted_dtype = promote_types(x.dtype(), y.dtype());\n"
"\n"
"    // Upcast to float32 for non-floating point inputs x and y\n"
"    auto out_dtype = is_floating_point(promoted_dtype)\n"
"        ? promoted_dtype\n"
"        : promote_types(promoted_dtype, float32);\n"
"\n"
"    // Cast x and y up to the determined dtype (on the same stream s)\n"
"    auto x_casted = astype(x, out_dtype, s);\n"
"    auto y_casted = astype(y, out_dtype, s);\n"
"\n"
"    // Broadcast the shapes of x and y (on the same stream s)\n"
"    auto broadcasted_inputs = broadcast_arrays({x_casted, y_casted}, s);\n"
"    auto out_shape = broadcasted_inputs[0].shape();\n"
"\n"
"    // Construct the array as the output of the Axpby primitive\n"
"    // with the broadcasted and upcasted arrays as inputs\n"
"    return array(\n"
"        /* const std::vector<int>& shape = */ out_shape,\n"
"        /* Dtype dtype = */ out_dtype,\n"
"        /* std::unique_ptr<Primitive> primitive = */\n"
"        std::make_shared<Axpby>(to_stream(s), alpha, beta),\n"
"        /* const std::vector<array>& inputs = */ broadcasted_inputs);\n"
"}"
msgstr ""

#: ../../../src/dev/extensions.rst:214
msgid "This operation now handles the following:"
msgstr ""

#: ../../../src/dev/extensions.rst:216
msgid "Upcast inputs and resolve the output data type."
msgstr ""

#: ../../../src/dev/extensions.rst:217
msgid "Broadcast the inputs and resolve the output shape."
msgstr ""

#: ../../../src/dev/extensions.rst:218
msgid ""
"Construct the primitive :class:`Axpby` using the given stream, ``alpha``, "
"and ``beta``."
msgstr ""

#: ../../../src/dev/extensions.rst:219
msgid "Construct the output :class:`array` using the primitive and the inputs."
msgstr ""

#: ../../../src/dev/extensions.rst:222
msgid "Implementing the Primitive"
msgstr ""

#: ../../../src/dev/extensions.rst:224
msgid ""
"No computation happens when we call the operation alone. The operation only "
"builds the computation graph. When we evaluate the output array, MLX "
"schedules the execution of the computation graph, and calls :meth:`Axpby::"
"eval_cpu` or :meth:`Axpby::eval_gpu` depending on the stream/device "
"specified by the user."
msgstr ""

#: ../../../src/dev/extensions.rst:230
msgid ""
"When :meth:`Primitive::eval_cpu` or :meth:`Primitive::eval_gpu` are called, "
"no memory has been allocated for the output array. It falls on the "
"implementation of these functions to allocate memory as needed."
msgstr ""

#: ../../../src/dev/extensions.rst:235
msgid "Implementing the CPU Back-end"
msgstr ""

#: ../../../src/dev/extensions.rst:237
msgid ""
"Let's start by implementing a naive and generic version of :meth:`Axpby::"
"eval_cpu`. We declared this as a private member function of :class:`Axpby` "
"earlier called :meth:`Axpby::eval`."
msgstr ""

#: ../../../src/dev/extensions.rst:241
msgid ""
"Our naive method will go over each element of the output array, find the "
"corresponding input elements of ``x`` and ``y`` and perform the operation "
"point-wise. This is captured in the templated function :meth:`axpby_impl`."
msgstr ""

#: ../../../src/dev/extensions.rst:245
msgid ""
"template <typename T>\n"
"void axpby_impl(\n"
"        const array& x,\n"
"        const array& y,\n"
"        array& out,\n"
"        float alpha_,\n"
"        float beta_) {\n"
"    // We only allocate memory when we are ready to fill the output\n"
"    // malloc_or_wait synchronously allocates available memory\n"
"    // There may be a wait executed here if the allocation is requested\n"
"    // under memory-pressured conditions\n"
"    out.set_data(allocator::malloc_or_wait(out.nbytes()));\n"
"\n"
"    // Collect input and output data pointers\n"
"    const T* x_ptr = x.data<T>();\n"
"    const T* y_ptr = y.data<T>();\n"
"    T* out_ptr = out.data<T>();\n"
"\n"
"    // Cast alpha and beta to the relevant types\n"
"    T alpha = static_cast<T>(alpha_);\n"
"    T beta = static_cast<T>(beta_);\n"
"\n"
"    // Do the element-wise operation for each output\n"
"    for (size_t out_idx = 0; out_idx < out.size(); out_idx++) {\n"
"        // Map linear indices to offsets in x and y\n"
"        auto x_offset = elem_to_loc(out_idx, x.shape(), x.strides());\n"
"        auto y_offset = elem_to_loc(out_idx, y.shape(), y.strides());\n"
"\n"
"        // We allocate the output to be contiguous and regularly strided\n"
"        // (defaults to row major) and hence it doesn't need additional "
"mapping\n"
"        out_ptr[out_idx] = alpha * x_ptr[x_offset] + beta * "
"y_ptr[y_offset];\n"
"    }\n"
"}"
msgstr ""

#: ../../../src/dev/extensions.rst:281
msgid ""
"Our implementation should work for all incoming floating point arrays. "
"Accordingly, we add dispatches for ``float32``, ``float16``, ``bfloat16`` "
"and ``complex64``. We throw an error if we encounter an unexpected type."
msgstr ""

#: ../../../src/dev/extensions.rst:285
msgid ""
"/** Fall back implementation for evaluation on CPU */\n"
"void Axpby::eval(\n"
"  const std::vector<array>& inputs,\n"
"  const std::vector<array>& outputs) {\n"
"    auto& x = inputs[0];\n"
"    auto& y = inputs[1];\n"
"    auto& out = outputs[0];\n"
"\n"
"    // Dispatch to the correct dtype\n"
"    if (out.dtype() == float32) {\n"
"        return axpby_impl<float>(x, y, out, alpha_, beta_);\n"
"    } else if (out.dtype() == float16) {\n"
"        return axpby_impl<float16_t>(x, y, out, alpha_, beta_);\n"
"    } else if (out.dtype() == bfloat16) {\n"
"        return axpby_impl<bfloat16_t>(x, y, out, alpha_, beta_);\n"
"    } else if (out.dtype() == complex64) {\n"
"        return axpby_impl<complex64_t>(x, y, out, alpha_, beta_);\n"
"    } else {\n"
"        throw std::runtime_error(\n"
"            \"[Axpby] Only supports floating point types.\");\n"
"    }\n"
"}"
msgstr ""

#: ../../../src/dev/extensions.rst:310
msgid ""
"This is good as a fallback implementation. We can use the ``axpby`` routine "
"provided by the Accelerate_ framework for a faster implementation in certain "
"cases:"
msgstr ""

#: ../../../src/dev/extensions.rst:314
msgid ""
"Accelerate does not provide implementations of ``axpby`` for half precision "
"floats. We can only use it for ``float32`` types."
msgstr ""

#: ../../../src/dev/extensions.rst:316
msgid ""
"Accelerate assumes the inputs ``x`` and ``y`` are contiguous and all "
"elements have fixed strides between them. We only direct to Accelerate if "
"both ``x`` and ``y`` are row contiguous or column contiguous."
msgstr ""

#: ../../../src/dev/extensions.rst:319
msgid ""
"Accelerate performs the routine ``Y = (alpha * X) + (beta * Y)`` in-place. "
"MLX expects to write the output to a new array. We must copy the elements of "
"``y`` into the output and use that as an input to ``axpby``."
msgstr ""

#: ../../../src/dev/extensions.rst:323
msgid ""
"Let's write an implementation that uses Accelerate in the right conditions. "
"It allocates data for the output, copies ``y`` into it, and then calls the :"
"func:`catlas_saxpby` from accelerate."
msgstr ""

#: ../../../src/dev/extensions.rst:327
msgid ""
"template <typename T>\n"
"void axpby_impl_accelerate(\n"
"        const array& x,\n"
"        const array& y,\n"
"        array& out,\n"
"        float alpha_,\n"
"        float beta_) {\n"
"    // Accelerate library provides catlas_saxpby which does\n"
"    // Y = (alpha * X) + (beta * Y) in place\n"
"    // To use it, we first copy the data in y over to the output array\n"
"    out.set_data(allocator::malloc_or_wait(out.nbytes()));\n"
"\n"
"    // We then copy over the elements using the contiguous vector "
"specialization\n"
"    copy_inplace(y, out, CopyType::Vector);\n"
"\n"
"    // Get x and y pointers for catlas_saxpby\n"
"    const T* x_ptr = x.data<T>();\n"
"    T* y_ptr = out.data<T>();\n"
"\n"
"    T alpha = static_cast<T>(alpha_);\n"
"    T beta = static_cast<T>(beta_);\n"
"\n"
"    // Call the inplace accelerate operator\n"
"    catlas_saxpby(\n"
"        /* N = */ out.size(),\n"
"        /* ALPHA = */ alpha,\n"
"        /* X = */ x_ptr,\n"
"        /* INCX = */ 1,\n"
"        /* BETA = */ beta,\n"
"        /* Y = */ y_ptr,\n"
"        /* INCY = */ 1);\n"
"}"
msgstr ""

#: ../../../src/dev/extensions.rst:362
msgid ""
"For inputs that do not fit the criteria for accelerate, we fall back to :"
"meth:`Axpby::eval`. With this in mind, let's finish our :meth:`Axpby::"
"eval_cpu`."
msgstr ""

#: ../../../src/dev/extensions.rst:366
msgid ""
"/** Evaluate primitive on CPU using accelerate specializations */\n"
"void Axpby::eval_cpu(\n"
"  const std::vector<array>& inputs,\n"
"  const std::vector<array>& outputs) {\n"
"    assert(inputs.size() == 2);\n"
"    auto& x = inputs[0];\n"
"    auto& y = inputs[1];\n"
"    auto& out = outputs[0];\n"
"\n"
"    // Accelerate specialization for contiguous single precision float "
"arrays\n"
"    if (out.dtype() == float32 &&\n"
"        ((x.flags().row_contiguous && y.flags().row_contiguous) ||\n"
"        (x.flags().col_contiguous && y.flags().col_contiguous))) {\n"
"        axpby_impl_accelerate<float>(x, y, out, alpha_, beta_);\n"
"        return;\n"
"    }\n"
"\n"
"    // Fall back to common back-end if specializations are not available\n"
"    eval(inputs, outputs);\n"
"}"
msgstr ""

#: ../../../src/dev/extensions.rst:389
msgid ""
"Just this much is enough to run the operation :meth:`axpby` on a CPU stream! "
"If you do not plan on running the operation on the GPU or using transforms "
"on computation graphs that contain :class:`Axpby`, you can stop implementing "
"the primitive here and enjoy the speed-ups you get from the Accelerate "
"library."
msgstr ""

#: ../../../src/dev/extensions.rst:395
msgid "Implementing the GPU Back-end"
msgstr ""

#: ../../../src/dev/extensions.rst:397
msgid ""
"Apple silicon devices address their GPUs using the Metal_ shading language, "
"and GPU kernels in MLX are written using Metal."
msgstr ""

#: ../../../src/dev/extensions.rst:402
msgid "Here are some helpful resources if you are new to Metal:"
msgstr ""

#: ../../../src/dev/extensions.rst:404
msgid "A walkthrough of the metal compute pipeline: `Metal Example`_"
msgstr ""

#: ../../../src/dev/extensions.rst:405
msgid "Documentation for metal shading language: `Metal Specification`_"
msgstr ""

#: ../../../src/dev/extensions.rst:406
msgid "Using metal from C++: `Metal-cpp`_"
msgstr ""

#: ../../../src/dev/extensions.rst:408
msgid ""
"Let's keep the GPU kernel simple. We will launch exactly as many threads as "
"there are elements in the output. Each thread will pick the element it needs "
"from ``x`` and ``y``, do the point-wise operation, and update its assigned "
"element in the output."
msgstr ""

#: ../../../src/dev/extensions.rst:413
msgid ""
"template <typename T>\n"
"[[kernel]] void axpby_general(\n"
"        device const T* x [[buffer(0)]],\n"
"        device const T* y [[buffer(1)]],\n"
"        device T* out [[buffer(2)]],\n"
"        constant const float& alpha [[buffer(3)]],\n"
"        constant const float& beta [[buffer(4)]],\n"
"        constant const int* shape [[buffer(5)]],\n"
"        constant const size_t* x_strides [[buffer(6)]],\n"
"        constant const size_t* y_strides [[buffer(7)]],\n"
"        constant const int& ndim [[buffer(8)]],\n"
"        uint index [[thread_position_in_grid]]) {\n"
"    // Convert linear indices to offsets in array\n"
"    auto x_offset = elem_to_loc(index, shape, x_strides, ndim);\n"
"    auto y_offset = elem_to_loc(index, shape, y_strides, ndim);\n"
"\n"
"    // Do the operation and update the output\n"
"    out[index] =\n"
"        static_cast<T>(alpha) * x[x_offset] + static_cast<T>(beta) * "
"y[y_offset];\n"
"}"
msgstr ""

#: ../../../src/dev/extensions.rst:436
msgid ""
"We then need to instantiate this template for all floating point types and "
"give each instantiation a unique host name so we can identify it."
msgstr ""

#: ../../../src/dev/extensions.rst:439
msgid ""
"#define instantiate_axpby(type_name, type)              \\\n"
"    template [[host_name(\"axpby_general_\" #type_name)]] \\\n"
"    [[kernel]] void axpby_general<type>(                \\\n"
"        device const type* x [[buffer(0)]],             \\\n"
"        device const type* y [[buffer(1)]],             \\\n"
"        device type* out [[buffer(2)]],                 \\\n"
"        constant const float& alpha [[buffer(3)]],      \\\n"
"        constant const float& beta [[buffer(4)]],       \\\n"
"        constant const int* shape [[buffer(5)]],        \\\n"
"        constant const size_t* x_strides [[buffer(6)]], \\\n"
"        constant const size_t* y_strides [[buffer(7)]], \\\n"
"        constant const int& ndim [[buffer(8)]],         \\\n"
"        uint index [[thread_position_in_grid]]);\n"
"\n"
"instantiate_axpby(float32, float);\n"
"instantiate_axpby(float16, half);\n"
"instantiate_axpby(bfloat16, bfloat16_t);\n"
"instantiate_axpby(complex64, complex64_t);"
msgstr ""

#: ../../../src/dev/extensions.rst:460
msgid ""
"The logic to determine the kernel, set the inputs, resolve the grid "
"dimensions, and dispatch to the GPU are contained in :meth:`Axpby::eval_gpu` "
"as shown below."
msgstr ""

#: ../../../src/dev/extensions.rst:464
msgid ""
"/** Evaluate primitive on GPU */\n"
"void Axpby::eval_gpu(\n"
"  const std::vector<array>& inputs,\n"
"  std::vector<array>& outputs) {\n"
"    // Prepare inputs\n"
"    assert(inputs.size() == 2);\n"
"    auto& x = inputs[0];\n"
"    auto& y = inputs[1];\n"
"    auto& out = outputs[0];\n"
"\n"
"    // Each primitive carries the stream it should execute on\n"
"    // and each stream carries its device identifiers\n"
"    auto& s = stream();\n"
"    // We get the needed metal device using the stream\n"
"    auto& d = metal::device(s.device);\n"
"\n"
"    // Allocate output memory\n"
"    out.set_data(allocator::malloc_or_wait(out.nbytes()));\n"
"\n"
"    // Resolve name of kernel\n"
"    std::ostringstream kname;\n"
"    kname << \"axpby_\" << \"general_\" << type_to_name(out);\n"
"\n"
"    // Make sure the metal library is available\n"
"    d.register_library(\"mlx_ext\");\n"
"\n"
"    // Make a kernel from this metal library\n"
"    auto kernel = d.get_kernel(kname.str(), \"mlx_ext\");\n"
"\n"
"    // Prepare to encode kernel\n"
"    auto& compute_encoder = d.get_command_encoder(s.index);\n"
"    compute_encoder->setComputePipelineState(kernel);\n"
"\n"
"    // Kernel parameters are registered with buffer indices corresponding "
"to\n"
"    // those in the kernel declaration at axpby.metal\n"
"    int ndim = out.ndim();\n"
"    size_t nelem = out.size();\n"
"\n"
"    // Encode input arrays to kernel\n"
"    compute_encoder.set_input_array(x, 0);\n"
"    compute_encoder.set_input_array(y, 1);\n"
"\n"
"    // Encode output arrays to kernel\n"
"    compute_encoder.set_output_array(out, 2);\n"
"\n"
"    // Encode alpha and beta\n"
"    compute_encoder->setBytes(&alpha_, sizeof(float), 3);\n"
"    compute_encoder->setBytes(&beta_, sizeof(float), 4);\n"
"\n"
"    // Encode shape, strides and ndim\n"
"    compute_encoder->setBytes(x.shape().data(), ndim * sizeof(int), 5);\n"
"    compute_encoder->setBytes(x.strides().data(), ndim * sizeof(size_t), "
"6);\n"
"    compute_encoder->setBytes(y.strides().data(), ndim * sizeof(size_t), "
"7);\n"
"    compute_encoder->setBytes(&ndim, sizeof(int), 8);\n"
"\n"
"    // We launch 1 thread for each input and make sure that the number of\n"
"    // threads in any given threadgroup is not higher than the max allowed\n"
"    size_t tgp_size = std::min(nelem, kernel-"
">maxTotalThreadsPerThreadgroup());\n"
"\n"
"    // Fix the 3D size of each threadgroup (in terms of threads)\n"
"    MTL::Size group_dims = MTL::Size(tgp_size, 1, 1);\n"
"\n"
"    // Fix the 3D size of the launch grid (in terms of threads)\n"
"    MTL::Size grid_dims = MTL::Size(nelem, 1, 1);\n"
"\n"
"    // Launch the grid with the given number of threads divided among\n"
"    // the given threadgroups\n"
"    compute_encoder.dispatchThreads(grid_dims, group_dims);\n"
"}"
msgstr ""

#: ../../../src/dev/extensions.rst:536
msgid ""
"We can now call the :meth:`axpby` operation on both the CPU and the GPU!"
msgstr ""

#: ../../../src/dev/extensions.rst:538
msgid ""
"A few things to note about MLX and Metal before moving on. MLX keeps track "
"of the active ``command_buffer`` and the ``MTLCommandBuffer`` to which it is "
"associated. We rely on :meth:`d.get_command_encoder` to give us the active "
"metal compute command encoder instead of building a new one and calling :"
"meth:`compute_encoder->end_encoding` at the end. MLX adds kernels (compute "
"pipelines) to the active command buffer until some specified limit is hit or "
"the command buffer needs to be flushed for synchronization."
msgstr ""

#: ../../../src/dev/extensions.rst:547
msgid "Primitive Transforms"
msgstr ""

#: ../../../src/dev/extensions.rst:549
msgid ""
"Next, let's add implementations for transformations in a :class:`Primitive`. "
"These transformations can be built on top of other operations, including the "
"one we just defined:"
msgstr ""

#: ../../../src/dev/extensions.rst:553
msgid ""
"/** The Jacobian-vector product. */\n"
"std::vector<array> Axpby::jvp(\n"
"        const std::vector<array>& primals,\n"
"        const std::vector<array>& tangents,\n"
"        const std::vector<int>& argnums) {\n"
"    // Forward mode diff that pushes along the tangents\n"
"    // The jvp transform on the primitive can built with ops\n"
"    // that are scheduled on the same stream as the primitive\n"
"\n"
"    // If argnums = {0}, we only push along x in which case the\n"
"    // jvp is just the tangent scaled by alpha\n"
"    // Similarly, if argnums = {1}, the jvp is just the tangent\n"
"    // scaled by beta\n"
"    if (argnums.size() > 1) {\n"
"        auto scale = argnums[0] == 0 ? alpha_ : beta_;\n"
"        auto scale_arr = array(scale, tangents[0].dtype());\n"
"        return {multiply(scale_arr, tangents[0], stream())};\n"
"    }\n"
"    // If, argnums = {0, 1}, we take contributions from both\n"
"    // which gives us jvp = tangent_x * alpha + tangent_y * beta\n"
"    else {\n"
"        return {axpby(tangents[0], tangents[1], alpha_, beta_, stream())};\n"
"    }\n"
"}"
msgstr ""

#: ../../../src/dev/extensions.rst:580
msgid ""
"/** The vector-Jacobian product. */\n"
"std::vector<array> Axpby::vjp(\n"
"        const std::vector<array>& primals,\n"
"        const std::vector<array>& cotangents,\n"
"        const std::vector<int>& argnums,\n"
"        const std::vector<int>& /* unused */) {\n"
"    // Reverse mode diff\n"
"    std::vector<array> vjps;\n"
"    for (auto arg : argnums) {\n"
"        auto scale = arg == 0 ? alpha_ : beta_;\n"
"        auto scale_arr = array(scale, cotangents[0].dtype());\n"
"        vjps.push_back(multiply(scale_arr, cotangents[0], stream()));\n"
"    }\n"
"    return vjps;\n"
"}"
msgstr ""

#: ../../../src/dev/extensions.rst:598
msgid ""
"Note, a transformation does not need to be fully defined to start using the :"
"class:`Primitive`."
msgstr ""

#: ../../../src/dev/extensions.rst:601
msgid ""
"/** Vectorize primitive along given axis */\n"
"std::pair<std::vector<array>, std::vector<int>> Axpby::vmap(\n"
"        const std::vector<array>& inputs,\n"
"        const std::vector<int>& axes) {\n"
"    throw std::runtime_error(\"[Axpby] vmap not implemented.\");\n"
"}"
msgstr ""

#: ../../../src/dev/extensions.rst:611
msgid "Building and Binding"
msgstr ""

#: ../../../src/dev/extensions.rst:613
msgid "Let's look at the overall directory structure first."
msgstr ""

#: ../../../src/dev/extensions.rst:615 ../../../src/dev/extensions.rst:794
msgid "extensions"
msgstr ""

#: ../../../src/dev/extensions.rst:616
msgid "├── axpby"
msgstr ""

#: ../../../src/dev/extensions.rst:617
msgid "│   ├── axpby.cpp"
msgstr ""

#: ../../../src/dev/extensions.rst:618
msgid "│   ├── axpby.h"
msgstr ""

#: ../../../src/dev/extensions.rst:619
msgid "│   └── axpby.metal"
msgstr ""

#: ../../../src/dev/extensions.rst:620 ../../../src/dev/extensions.rst:795
msgid "├── mlx_sample_extensions"
msgstr ""

#: ../../../src/dev/extensions.rst:621
msgid "│   └── __init__.py"
msgstr ""

#: ../../../src/dev/extensions.rst:622
msgid "├── bindings.cpp"
msgstr ""

#: ../../../src/dev/extensions.rst:623
msgid "├── CMakeLists.txt"
msgstr ""

#: ../../../src/dev/extensions.rst:624
msgid "└── setup.py"
msgstr ""

#: ../../../src/dev/extensions.rst:626
msgid "``extensions/axpby/`` defines the C++ extension library"
msgstr ""

#: ../../../src/dev/extensions.rst:627
msgid ""
"``extensions/mlx_sample_extensions`` sets out the structure for the "
"associated Python package"
msgstr ""

#: ../../../src/dev/extensions.rst:629
msgid "``extensions/bindings.cpp`` provides Python bindings for our operation"
msgstr ""

#: ../../../src/dev/extensions.rst:630
msgid ""
"``extensions/CMakeLists.txt`` holds CMake rules to build the library and "
"Python bindings"
msgstr ""

#: ../../../src/dev/extensions.rst:632
msgid ""
"``extensions/setup.py`` holds the ``setuptools`` rules to build and install "
"the Python package"
msgstr ""

#: ../../../src/dev/extensions.rst:636
msgid "Binding to Python"
msgstr ""

#: ../../../src/dev/extensions.rst:638
msgid ""
"We use nanobind_ to build a Python API for the C++ library. Since bindings "
"for components such as :class:`mlx.core.array`, :class:`mlx.core.stream`, "
"etc. are already provided, adding our :meth:`axpby` is simple."
msgstr ""

#: ../../../src/dev/extensions.rst:642
msgid ""
"NB_MODULE(_ext, m) {\n"
"     m.doc() = \"Sample extension for MLX\";\n"
"\n"
"     m.def(\n"
"         \"axpby\",\n"
"         &axpby,\n"
"         \"x\"_a,\n"
"         \"y\"_a,\n"
"         \"alpha\"_a,\n"
"         \"beta\"_a,\n"
"         nb::kw_only(),\n"
"         \"stream\"_a = nb::none(),\n"
"         R\"(\n"
"             Scale and sum two vectors element-wise\n"
"             ``z = alpha * x + beta * y``\n"
"\n"
"             Follows numpy style broadcasting between ``x`` and ``y``\n"
"             Inputs are upcasted to floats if needed\n"
"\n"
"             Args:\n"
"                 x (array): Input array.\n"
"                 y (array): Input array.\n"
"                 alpha (float): Scaling factor for ``x``.\n"
"                 beta (float): Scaling factor for ``y``.\n"
"\n"
"             Returns:\n"
"                 array: ``alpha * x + beta * y``\n"
"         )\");\n"
" }"
msgstr ""

#: ../../../src/dev/extensions.rst:674
msgid ""
"Most of the complexity in the above example comes from additional bells and "
"whistles such as the literal names and doc-strings."
msgstr ""

#: ../../../src/dev/extensions.rst:679
msgid ""
":mod:`mlx.core` must be imported before importing :mod:"
"`mlx_sample_extensions` as defined by the nanobind module above to ensure "
"that the casters for :mod:`mlx.core` components like :class:`mlx.core.array` "
"are available."
msgstr ""

#: ../../../src/dev/extensions.rst:687
msgid "Building with CMake"
msgstr ""

#: ../../../src/dev/extensions.rst:689
msgid ""
"Building the C++ extension library only requires that you ``find_package(MLX "
"CONFIG)`` and then link it to your library."
msgstr ""

#: ../../../src/dev/extensions.rst:692
msgid ""
"# Add library\n"
"add_library(mlx_ext)\n"
"\n"
"# Add sources\n"
"target_sources(\n"
"    mlx_ext\n"
"    PUBLIC\n"
"    ${CMAKE_CURRENT_LIST_DIR}/axpby/axpby.cpp\n"
")\n"
"\n"
"# Add include headers\n"
"target_include_directories(\n"
"    mlx_ext PUBLIC ${CMAKE_CURRENT_LIST_DIR}\n"
")\n"
"\n"
"# Link to mlx\n"
"target_link_libraries(mlx_ext PUBLIC mlx)"
msgstr ""

#: ../../../src/dev/extensions.rst:712
msgid ""
"We also need to build the attached Metal library. For convenience, we "
"provide a :meth:`mlx_build_metallib` function that builds a ``.metallib`` "
"target given sources, headers, destinations, etc. (defined in ``cmake/"
"extension.cmake`` and automatically imported with MLX package)."
msgstr ""

#: ../../../src/dev/extensions.rst:717
msgid "Here is what that looks like in practice:"
msgstr ""

#: ../../../src/dev/extensions.rst:719
msgid ""
"# Build metallib\n"
"if(MLX_BUILD_METAL)\n"
"\n"
"mlx_build_metallib(\n"
"    TARGET mlx_ext_metallib\n"
"    TITLE mlx_ext\n"
"    SOURCES ${CMAKE_CURRENT_LIST_DIR}/axpby/axpby.metal\n"
"    INCLUDE_DIRS ${PROJECT_SOURCE_DIR} ${MLX_INCLUDE_DIRS}\n"
"    OUTPUT_DIRECTORY ${CMAKE_LIBRARY_OUTPUT_DIRECTORY}\n"
")\n"
"\n"
"add_dependencies(\n"
"    mlx_ext\n"
"    mlx_ext_metallib\n"
")\n"
"\n"
"endif()"
msgstr ""

#: ../../../src/dev/extensions.rst:739
msgid "Finally, we build the nanobind_ bindings"
msgstr ""

#: ../../../src/dev/extensions.rst:741
msgid ""
"nanobind_add_module(\n"
"  _ext\n"
"  NB_STATIC STABLE_ABI LTO NOMINSIZE\n"
"  NB_DOMAIN mlx\n"
"  ${CMAKE_CURRENT_LIST_DIR}/bindings.cpp\n"
")\n"
"target_link_libraries(_ext PRIVATE mlx_ext)\n"
"\n"
"if(BUILD_SHARED_LIBS)\n"
"  target_link_options(_ext PRIVATE -Wl,-rpath,@loader_path)\n"
"endif()"
msgstr ""

#: ../../../src/dev/extensions.rst:756
msgid "Building with ``setuptools``"
msgstr ""

#: ../../../src/dev/extensions.rst:758
msgid ""
"Once we have set out the CMake build rules as described above, we can use "
"the build utilities defined in :mod:`mlx.extension`:"
msgstr ""

#: ../../../src/dev/extensions.rst:761
msgid ""
"from mlx import extension\n"
"from setuptools import setup\n"
"\n"
"if __name__ == \"__main__\":\n"
"    setup(\n"
"        name=\"mlx_sample_extensions\",\n"
"        version=\"0.0.0\",\n"
"        description=\"Sample C++ and Metal extensions for MLX primitives."
"\",\n"
"        ext_modules=[extension.CMakeExtension(\"mlx_sample_extensions."
"_ext\")],\n"
"        cmdclass={\"build_ext\": extension.CMakeBuild},\n"
"        packages=[\"mlx_sample_extensions\"],\n"
"        package_data={\"mlx_sample_extensions\": [\"*.so\", \"*.dylib\", \"*."
"metallib\"]},\n"
"        extras_require={\"dev\":[]},\n"
"        zip_safe=False,\n"
"        python_requires=\">=3.8\",\n"
"    )"
msgstr ""

#: ../../../src/dev/extensions.rst:781
msgid ""
"We treat ``extensions/mlx_sample_extensions`` as the package directory even "
"though it only contains a ``__init__.py`` to ensure the following:"
msgstr ""

#: ../../../src/dev/extensions.rst:784
msgid ":mod:`mlx.core` must be imported before importing :mod:`_ext`"
msgstr ""

#: ../../../src/dev/extensions.rst:785
msgid ""
"The C++ extension library and the metal library are co-located with the "
"python bindings and copied together if the package is installed"
msgstr ""

#: ../../../src/dev/extensions.rst:788
msgid ""
"To build the package, first install the build dependencies with ``pip "
"install -r requirements.txt``.  You can then build inplace for development "
"using ``python setup.py build_ext -j8 --inplace`` (in ``extensions/``)"
msgstr ""

#: ../../../src/dev/extensions.rst:792
msgid "This results in the directory structure:"
msgstr ""

#: ../../../src/dev/extensions.rst:796
msgid "│   ├── __init__.py"
msgstr ""

#: ../../../src/dev/extensions.rst:797
msgid "│   ├── libmlx_ext.dylib # C++ extension library"
msgstr ""

#: ../../../src/dev/extensions.rst:798
msgid "│   ├── mlx_ext.metallib # Metal library"
msgstr ""

#: ../../../src/dev/extensions.rst:799
msgid "│   └── _ext.cpython-3x-darwin.so # Python Binding"
msgstr ""

#: ../../../src/dev/extensions.rst:800
msgid "..."
msgstr ""

#: ../../../src/dev/extensions.rst:802
msgid ""
"When you try to install using the command ``python -m pip install .`` (in "
"``extensions/``), the package will be installed with the same structure as "
"``extensions/mlx_sample_extensions`` and the C++ and Metal library will be "
"copied along with the Python binding since they are specified as "
"``package_data``."
msgstr ""

#: ../../../src/dev/extensions.rst:809
msgid "Usage"
msgstr ""

#: ../../../src/dev/extensions.rst:811
msgid ""
"After installing the extension as described above, you should be able to "
"simply import the Python package and play with it as you would any other MLX "
"operation."
msgstr ""

#: ../../../src/dev/extensions.rst:814
msgid "Let's look at a simple script and its results:"
msgstr ""

#: ../../../src/dev/extensions.rst:816
msgid ""
"import mlx.core as mx\n"
"from mlx_sample_extensions import axpby\n"
"\n"
"a = mx.ones((3, 4))\n"
"b = mx.ones((3, 4))\n"
"c = axpby(a, b, 4.0, 2.0, stream=mx.cpu)\n"
"\n"
"print(f\"c shape: {c.shape}\")\n"
"print(f\"c dtype: {c.dtype}\")\n"
"print(f\"c correct: {mx.all(c == 6.0).item()}\")"
msgstr ""

#: ../../../src/dev/extensions.rst:829
msgid "Output:"
msgstr ""

#: ../../../src/dev/extensions.rst:831
msgid ""
"c shape: [3, 4]\n"
"c dtype: float32\n"
"c correctness: True"
msgstr ""

#: ../../../src/dev/extensions.rst:838
msgid "Results"
msgstr ""

#: ../../../src/dev/extensions.rst:840
msgid ""
"Let's run a quick benchmark and see how our new ``axpby`` operation compares "
"with the naive :meth:`simple_axpby` we first defined on the CPU."
msgstr ""

#: ../../../src/dev/extensions.rst:843
msgid ""
"import mlx.core as mx\n"
"from mlx_sample_extensions import axpby\n"
"import time\n"
"\n"
"mx.set_default_device(mx.cpu)\n"
"\n"
"def simple_axpby(x: mx.array, y: mx.array, alpha: float, beta: float) -> mx."
"array:\n"
"    return alpha * x + beta * y\n"
"\n"
"M = 256\n"
"N = 512\n"
"\n"
"x = mx.random.normal((M, N))\n"
"y = mx.random.normal((M, N))\n"
"alpha = 4.0\n"
"beta = 2.0\n"
"\n"
"mx.eval(x, y)\n"
"\n"
"def bench(f):\n"
"    # Warm up\n"
"    for i in range(100):\n"
"        z = f(x, y, alpha, beta)\n"
"        mx.eval(z)\n"
"\n"
"    # Timed run\n"
"    s = time.time()\n"
"    for i in range(5000):\n"
"        z = f(x, y, alpha, beta)\n"
"        mx.eval(z)\n"
"    e = time.time()\n"
"    return e - s\n"
"\n"
"simple_time = bench(simple_axpby)\n"
"custom_time = bench(axpby)\n"
"\n"
"print(f\"Simple axpby: {simple_time:.3f} s | Custom axpby: {custom_time:.3f} "
"s\")"
msgstr ""

#: ../../../src/dev/extensions.rst:883
msgid ""
"The results are ``Simple axpby: 0.114 s | Custom axpby: 0.109 s``. We see "
"modest improvements right away!"
msgstr ""

#: ../../../src/dev/extensions.rst:886
msgid ""
"This operation is now good to be used to build other operations, in :class:"
"`mlx.nn.Module` calls, and also as a part of graph transformations like :"
"meth:`grad`."
msgstr ""

#: ../../../src/dev/extensions.rst:891
msgid "Scripts"
msgstr ""

#: ../../../src/dev/extensions.rst:893
msgid "Download the code"
msgstr ""

#: ../../../src/dev/extensions.rst:895
msgid ""
"The full example code is available in `mlx <https://github.com/ml-explore/"
"mlx/tree/main/examples/extensions/>`_."
msgstr ""
