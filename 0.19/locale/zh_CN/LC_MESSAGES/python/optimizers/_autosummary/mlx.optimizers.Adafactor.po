# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2023, MLX Contributors
# This file is distributed under the same license as the MLX package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: MLX 0.19\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-12-08 12:31+0000\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"Language: zh_CN\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"

#: ../../../src/python/optimizers/_autosummary/mlx.optimizers.Adafactor.rst:2
msgid "mlx.optimizers.Adafactor"
msgstr ""

#: ../../../../../../.conda/lib/python3.12/site-packages/mlx/optimizers/optimizers.py:docstring
#: of mlx.optimizers.optimizers.Adafactor:1
msgid "The Adafactor optimizer."
msgstr ""

#: ../../../../../../.conda/lib/python3.12/site-packages/mlx/optimizers/optimizers.py:docstring
#: of mlx.optimizers.optimizers.Adafactor:3
msgid ""
"Our Adafactor implementation follows the original paper: `Adafactor: "
"Adaptive Learning Rates with Sublinear Memory Cost <https://arxiv.org/"
"abs/1804.04235>`_"
msgstr ""

#: ../../../src/python/optimizers/_autosummary/mlx.optimizers.Adafactor.rst:0
msgid "Parameters"
msgstr ""

#: ../../../../../../.conda/lib/python3.12/site-packages/mlx/optimizers/optimizers.py:docstring
#: of mlx.optimizers.optimizers.Adafactor:7
msgid "The learning rate. Default: ``None``."
msgstr ""

#: ../../../../../../.conda/lib/python3.12/site-packages/mlx/optimizers/optimizers.py:docstring
#: of mlx.optimizers.optimizers.Adafactor:10
msgid ""
"The first term :math:`\\epsilon_1` added to the square of the gradients to "
"improve numerical stability and the second term :math:`\\epsilon_2` is used "
"for parameter scaling if ``parameter_scale`` is set to ``True``. Default: "
"``(1e-30, 1e-3)``."
msgstr ""

#: ../../../../../../.conda/lib/python3.12/site-packages/mlx/optimizers/optimizers.py:docstring
#: of mlx.optimizers.optimizers.Adafactor:16
msgid "Clips the unscaled update at ``clip_threshold``. Default: ``1.0``."
msgstr ""

#: ../../../../../../.conda/lib/python3.12/site-packages/mlx/optimizers/optimizers.py:docstring
#: of mlx.optimizers.optimizers.Adafactor:19
msgid ""
"Coefficient for the running average of the squared gradient. Default: "
"``-0.8``."
msgstr ""

#: ../../../../../../.conda/lib/python3.12/site-packages/mlx/optimizers/optimizers.py:docstring
#: of mlx.optimizers.optimizers.Adafactor:22
msgid ""
"If set to a value bigger than zero then first moment will be used. Default: "
"``None``."
msgstr ""

#: ../../../../../../.conda/lib/python3.12/site-packages/mlx/optimizers/optimizers.py:docstring
#: of mlx.optimizers.optimizers.Adafactor:25
msgid "The weight decay :math:`\\lambda`. Default: ``0.0``."
msgstr ""

#: ../../../../../../.conda/lib/python3.12/site-packages/mlx/optimizers/optimizers.py:docstring
#: of mlx.optimizers.optimizers.Adafactor:28
msgid ""
"If set to ``True`` the learning rate will be scaled by :math:"
"`\\max(\\epsilon_1, \\text{RMS}(w_{t-1}))`. Default: ``True``."
msgstr ""

#: ../../../../../../.conda/lib/python3.12/site-packages/mlx/optimizers/optimizers.py:docstring
#: of mlx.optimizers.optimizers.Adafactor:32
msgid ""
"If set to ``True`` the ``learning_rate`` will be ignored and relative step "
"size will be computed. Default: ``True``."
msgstr ""

#: ../../../../../../.conda/lib/python3.12/site-packages/mlx/optimizers/optimizers.py:docstring
#: of mlx.optimizers.optimizers.Adafactor:36
msgid ""
"If set to ``True`` then the relative step size will be calculated by the "
"current step. Default: ``False``."
msgstr ""

#: ../../../src/python/optimizers/_autosummary/mlx.optimizers.Adafactor.rst:12
msgid "Methods"
msgstr ""

#: ../../../src/python/optimizers/_autosummary/mlx.optimizers.Adafactor.rst:17:<autosummary>:1
msgid ""
":py:obj:`__init__ <mlx.optimizers.Adafactor.__init__>`\\ "
"\\(\\[learning\\_rate\\, eps\\, ...\\]\\)"
msgstr ""

#: ../../../src/python/optimizers/_autosummary/mlx.optimizers.Adafactor.rst:17:<autosummary>:1
msgid ""
":py:obj:`apply_single <mlx.optimizers.Adafactor.apply_single>`\\ "
"\\(gradient\\, parameter\\, state\\)"
msgstr ""

#: ../../../src/python/optimizers/_autosummary/mlx.optimizers.Adafactor.rst:17:<autosummary>:1
msgid "Performs the Adafactor parameter and state update."
msgstr ""

#: ../../../src/python/optimizers/_autosummary/mlx.optimizers.Adafactor.rst:17:<autosummary>:1
msgid ""
":py:obj:`init_single <mlx.optimizers.Adafactor.init_single>`\\ "
"\\(parameter\\, state\\)"
msgstr ""

#: ../../../src/python/optimizers/_autosummary/mlx.optimizers.Adafactor.rst:17:<autosummary>:1
msgid "Initialize optimizer state"
msgstr ""
