# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2023, MLX Contributors
# This file is distributed under the same license as the MLX package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: MLX 0.8\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-12-08 12:57+0000\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"Language: zh_CN\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"

#: ../../../src/python/optimizers/_autosummary/mlx.optimizers.AdamW.rst:2
msgid "mlx.optimizers.AdamW"
msgstr ""

#: ../../../../../../.conda/lib/python3.12/site-packages/mlx/optimizers/optimizers.py:docstring
#: of mlx.optimizers.optimizers.AdamW:1
msgid "The AdamW optimizer [1]."
msgstr ""

#: ../../../../../../.conda/lib/python3.12/site-packages/mlx/optimizers/optimizers.py:docstring
#: of mlx.optimizers.optimizers.AdamW:3
msgid ""
"Following the above convention, in contrast with [1], we do not use bias "
"correction in the first and second moments for AdamW. We update the weights "
"with a weight_decay (:math:`\\lambda`) value:"
msgstr ""

#: ../../../../../../.conda/lib/python3.12/site-packages/mlx/optimizers/optimizers.py:docstring
#: of mlx.optimizers.optimizers.AdamW:7
msgid ""
"[1]: Loshchilov, I. and Hutter, F., 2019. Decoupled weight decay "
"regularization. ICLR 2019."
msgstr ""

#: ../../../../../../.conda/lib/python3.12/site-packages/mlx/optimizers/optimizers.py:docstring
#: of mlx.optimizers.optimizers.AdamW:10
msgid ""
"m_{t+1} &= \\beta_1 m_t + (1 - \\beta_1) g_t \\\\\n"
"v_{t+1} &= \\beta_2 v_t + (1 - \\beta_2) g_t^2 \\\\\n"
"w_{t+1} &= w_t - \\alpha (\\frac{m_{t+1}}{\\sqrt{v_{t+1} + \\epsilon}} + "
"\\lambda w_t)"
msgstr ""

#: ../../../src/python/optimizers/_autosummary/mlx.optimizers.AdamW.rst:0
msgid "Parameters"
msgstr ""

#: ../../../../../../.conda/lib/python3.12/site-packages/mlx/optimizers/optimizers.py:docstring
#: of mlx.optimizers.optimizers.AdamW:16
msgid "The learning rate :math:`\\alpha`."
msgstr ""

#: ../../../../../../.conda/lib/python3.12/site-packages/mlx/optimizers/optimizers.py:docstring
#: of mlx.optimizers.optimizers.AdamW:18
msgid ""
"The coefficients :math:`(\\beta_1, \\beta_2)` used for computing running "
"averages of the gradient and its square. Default: ``(0.9, 0.999)``"
msgstr ""

#: ../../../../../../.conda/lib/python3.12/site-packages/mlx/optimizers/optimizers.py:docstring
#: of mlx.optimizers.optimizers.AdamW:22
msgid ""
"The term :math:`\\epsilon` added to the denominator to improve numerical "
"stability. Default: ``1e-8``"
msgstr ""

#: ../../../../../../.conda/lib/python3.12/site-packages/mlx/optimizers/optimizers.py:docstring
#: of mlx.optimizers.optimizers.AdamW:25
msgid "The weight decay :math:`\\lambda`. Default: ``0``."
msgstr ""

#: ../../../src/python/optimizers/_autosummary/mlx.optimizers.AdamW.rst:12
msgid "Methods"
msgstr ""

#: ../../../src/python/optimizers/_autosummary/mlx.optimizers.AdamW.rst:16:<autosummary>:1
msgid ""
":py:obj:`__init__ <mlx.optimizers.AdamW.__init__>`\\ "
"\\(learning\\_rate\\[\\, betas\\, eps\\, ...\\]\\)"
msgstr ""

#: ../../../src/python/optimizers/_autosummary/mlx.optimizers.AdamW.rst:16:<autosummary>:1
msgid ""
":py:obj:`apply_single <mlx.optimizers.AdamW.apply_single>`\\ \\(gradient\\, "
"parameter\\, state\\)"
msgstr ""

#: ../../../src/python/optimizers/_autosummary/mlx.optimizers.AdamW.rst:16:<autosummary>:1
msgid ""
"Performs the AdamW parameter update by modifying the parameters passed into "
"Adam."
msgstr ""
