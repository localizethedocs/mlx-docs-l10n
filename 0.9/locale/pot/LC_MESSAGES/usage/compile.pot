# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2023, MLX Contributors
# This file is distributed under the same license as the MLX package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: MLX 0.9\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-12-08 13:00+0000\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"

#: ../../../src/usage/compile.rst:4
msgid "Compilation"
msgstr ""

#: ../../../src/usage/compile.rst:8
msgid ""
"MLX has a :func:`compile` function transformation which compiles computation "
"graphs. Function compilation results in smaller graphs by merging common "
"work and fusing certain operations. In many cases this can lead to big "
"improvements in run-time and memory use."
msgstr ""

#: ../../../src/usage/compile.rst:13
msgid ""
"Getting started with :func:`compile` is simple, but there are some edge "
"cases that are good to be aware of for more complex graphs and advanced "
"usage."
msgstr ""

#: ../../../src/usage/compile.rst:17
msgid "Basics of Compile"
msgstr ""

#: ../../../src/usage/compile.rst:19
msgid "Let's start with a simple example:"
msgstr ""

#: ../../../src/usage/compile.rst:21
msgid ""
"def fun(x, y):\n"
"    return mx.exp(-x) + y\n"
"\n"
"x = mx.array(1.0)\n"
"y = mx.array(2.0)\n"
"\n"
"# Regular call, no compilation\n"
"# Prints: array(2.36788, dtype=float32)\n"
"print(fun(x, y))\n"
"\n"
"# Compile the function\n"
"compiled_fun = mx.compile(fun)\n"
"\n"
"# Prints: array(2.36788, dtype=float32)\n"
"print(compiled_fun(x, y))"
msgstr ""

#: ../../../src/usage/compile.rst:39
msgid ""
"The output of both the regular function and the compiled function is the "
"same up to numerical precision."
msgstr ""

#: ../../../src/usage/compile.rst:42
msgid ""
"The first time you call a compiled function, MLX will build the compute "
"graph, optimize it, and generate and compile code. This can be relatively "
"slow. However, MLX will cache compiled functions, so calling a compiled "
"function multiple times will not initiate a new compilation. This means you "
"should typically compile functions that you plan to use more than once."
msgstr ""

#: ../../../src/usage/compile.rst:48
msgid ""
"def fun(x, y):\n"
"    return mx.exp(-x) + y\n"
"\n"
"x = mx.array(1.0)\n"
"y = mx.array(2.0)\n"
"\n"
"compiled_fun = mx.compile(fun)\n"
"\n"
"# Compiled here\n"
"compiled_fun(x, y)\n"
"\n"
"# Not compiled again\n"
"compiled_fun(x, y)\n"
"\n"
"# Not compiled again\n"
"mx.compile(fun)(x, y)"
msgstr ""

#: ../../../src/usage/compile.rst:67
msgid ""
"There are some important cases to be aware of that can cause a function to "
"be recompiled:"
msgstr ""

#: ../../../src/usage/compile.rst:70
msgid "Changing the shape or number of dimensions"
msgstr ""

#: ../../../src/usage/compile.rst:71
msgid "Changing the type of any of the inputs"
msgstr ""

#: ../../../src/usage/compile.rst:72
msgid "Changing the number of inputs to the function"
msgstr ""

#: ../../../src/usage/compile.rst:74
msgid ""
"In certain cases only some of the compilation stack will be rerun (for "
"example when changing the shapes) and in other cases the full compilation "
"stack will be rerun (for example when changing the types). In general you "
"should avoid compiling functions too frequently."
msgstr ""

#: ../../../src/usage/compile.rst:79
msgid ""
"Another idiom to watch out for is compiling functions which get created and "
"destroyed frequently. This can happen, for example, when compiling an "
"anonymous function in a loop:"
msgstr ""

#: ../../../src/usage/compile.rst:83
msgid ""
"a = mx.array(1.0)\n"
"# Don't do this, compiles lambda at each iteration\n"
"for _ in range(5):\n"
"    mx.compile(lambda x: mx.exp(mx.abs(x)))(a)"
msgstr ""

#: ../../../src/usage/compile.rst:91
msgid "Example Speedup"
msgstr ""

#: ../../../src/usage/compile.rst:93
msgid ""
"The :func:`mlx.nn.gelu` is a nonlinear activation function commonly used "
"with Transformer-based models. The implementation involves several unary and "
"binary element-wise operations:"
msgstr ""

#: ../../../src/usage/compile.rst:97
msgid ""
"def gelu(x):\n"
"    return x * (1 + mx.erf(x / math.sqrt(2))) / 2"
msgstr ""

#: ../../../src/usage/compile.rst:102
msgid ""
"If you use this function with small arrays, it will be overhead bound. If "
"you use it with large arrays it will be memory bandwidth bound.  However, "
"all of the operations in the ``gelu`` are fusible into a single kernel with :"
"func:`compile`. This can speedup both cases considerably."
msgstr ""

#: ../../../src/usage/compile.rst:107
msgid ""
"Let's compare the runtime of the regular function versus the compiled "
"function. We'll use the following timing helper which does a warm up and "
"handles synchronization:"
msgstr ""

#: ../../../src/usage/compile.rst:111
msgid ""
"import time\n"
"\n"
"def timeit(fun, x):\n"
"    # warm up\n"
"    for _ in range(10):\n"
"        mx.eval(fun(x))\n"
"\n"
"    tic = time.perf_counter()\n"
"    for _ in range(100):\n"
"        mx.eval(fun(x))\n"
"    toc = time.perf_counter()\n"
"    tpi = 1e3 * (toc - tic) / 100\n"
"    print(f\"Time per iteration {tpi:.3f} (ms)\")"
msgstr ""

#: ../../../src/usage/compile.rst:128
msgid "Now make an array, and benchmark both functions:"
msgstr ""

#: ../../../src/usage/compile.rst:130
msgid ""
"x = mx.random.uniform(shape=(32, 1000, 4096))\n"
"timeit(nn.gelu, x)\n"
"timeit(mx.compile(nn.gelu), x)"
msgstr ""

#: ../../../src/usage/compile.rst:136
msgid ""
"On an M1 Max the times are 15.5 and 3.1 milliseconds. The compiled ``gelu`` "
"is five times faster."
msgstr ""

#: ../../../src/usage/compile.rst:141
msgid ""
"As of the latest MLX, CPU functions are not fully compiled. Compiling CPU "
"functions can still be helpful, but won't typically result in as large a "
"speedup as compiling operations that run on the GPU."
msgstr ""

#: ../../../src/usage/compile.rst:147
msgid "Debugging"
msgstr ""

#: ../../../src/usage/compile.rst:149
msgid ""
"When a compiled function is first called, it is traced with placeholder "
"inputs. This means you can't evaluate arrays (for example to print their "
"contents) inside compiled functions."
msgstr ""

#: ../../../src/usage/compile.rst:153
msgid ""
"@mx.compile\n"
"def fun(x):\n"
"    z = -x\n"
"    print(z)  # Crash\n"
"    return mx.exp(z)\n"
"\n"
"fun(mx.array(5.0))"
msgstr ""

#: ../../../src/usage/compile.rst:163
msgid ""
"For debugging, inspecting arrays can be helpful. One way to do that is to "
"globally disable compilation using the :func:`disable_compile` function or "
"``MLX_DISABLE_COMPILE`` flag. For example the following is okay even though "
"``fun`` is compiled:"
msgstr ""

#: ../../../src/usage/compile.rst:168
msgid ""
"@mx.compile\n"
"def fun(x):\n"
"    z = -x\n"
"    print(z) # Okay\n"
"    return mx.exp(z)\n"
"\n"
"mx.disable_compile()\n"
"fun(mx.array(5.0))"
msgstr ""

#: ../../../src/usage/compile.rst:181
msgid "Pure Functions"
msgstr ""

#: ../../../src/usage/compile.rst:183
msgid ""
"Compiled functions are intended to be *pure*; that is they should not have "
"side effects. For example:"
msgstr ""

#: ../../../src/usage/compile.rst:186
msgid ""
"state = []\n"
"\n"
"@mx.compile\n"
"def fun(x, y):\n"
"    z = x + y\n"
"    state.append(z)\n"
"    return mx.exp(z)\n"
"\n"
"fun(mx.array(1.0), mx.array(2.0))\n"
"# Crash!\n"
"print(state)"
msgstr ""

#: ../../../src/usage/compile.rst:200
msgid ""
"After the first call of ``fun``, the ``state`` list will hold a placeholder "
"array. The placeholder does not have any data; it is only used to build the "
"computation graph. Printing such an array results in a crash."
msgstr ""

#: ../../../src/usage/compile.rst:204
msgid ""
"You have two options to deal with this. The first option is to simply return "
"``state`` as an output:"
msgstr ""

#: ../../../src/usage/compile.rst:207
msgid ""
"state = []\n"
"\n"
"@mx.compile\n"
"def fun(x, y):\n"
"   z = x + y\n"
"   state.append(z)\n"
"   return mx.exp(z), state\n"
"\n"
" _, state = fun(mx.array(1.0), mx.array(2.0))\n"
" # Prints [array(3, dtype=float32)]\n"
" print(state)"
msgstr ""

#: ../../../src/usage/compile.rst:221
msgid ""
"In some cases returning updated state can be pretty inconvenient. Hence, :"
"func:`compile` has a parameter to capture implicit outputs:"
msgstr ""

#: ../../../src/usage/compile.rst:224
msgid ""
"from functools import partial\n"
"\n"
"state = []\n"
"\n"
"# Tell compile to capture state as an output\n"
"@partial(mx.compile, outputs=state)\n"
"def fun(x, y):\n"
"    z = x + y\n"
"    state.append(z)\n"
"    return mx.exp(z), state\n"
"\n"
"fun(mx.array(1.0), mx.array(2.0))\n"
"# Prints [array(3, dtype=float32)]\n"
"print(state)"
msgstr ""

#: ../../../src/usage/compile.rst:241
msgid ""
"This is particularly useful for compiling a function which includes an "
"update to a container of arrays, as is commonly done when training the "
"parameters of a :class:`mlx.nn.Module`."
msgstr ""

#: ../../../src/usage/compile.rst:245
msgid ""
"Compiled functions will also treat any inputs not in the parameter list as "
"constants. For example:"
msgstr ""

#: ../../../src/usage/compile.rst:248
msgid ""
"state = [mx.array(1.0)]\n"
"\n"
"@mx.compile\n"
"def fun(x):\n"
"    return x + state[0]\n"
"\n"
"# Prints array(2, dtype=float32)\n"
"print(fun(mx.array(1.0)))\n"
"\n"
"# Update state\n"
"state[0] = mx.array(5.0)\n"
"\n"
"# Still prints array(2, dtype=float32)\n"
"print(fun(mx.array(1.0)))"
msgstr ""

#: ../../../src/usage/compile.rst:265
msgid ""
"In order to have the change of state reflected in the outputs of ``fun`` you "
"again have two options. The first option is to simply pass ``state`` as "
"input to the function. In some cases this can be pretty inconvenient. "
"Hence, :func:`compile` also has a parameter to capture implicit inputs:"
msgstr ""

#: ../../../src/usage/compile.rst:270
msgid ""
"from functools import partial\n"
"state = [mx.array(1.0)]\n"
"\n"
"# Tell compile to capture state as an input\n"
"@partial(mx.compile, inputs=state)\n"
"def fun(x):\n"
"    return x + state[0]\n"
"\n"
"# Prints array(2, dtype=float32)\n"
"print(fun(mx.array(1.0)))\n"
"\n"
"# Update state\n"
"state[0] = mx.array(5.0)\n"
"\n"
"# Prints array(6, dtype=float32)\n"
"print(fun(mx.array(1.0)))"
msgstr ""

#: ../../../src/usage/compile.rst:291
msgid "Compiling Training Graphs"
msgstr ""

#: ../../../src/usage/compile.rst:293
msgid ""
"This section will step through how to use :func:`compile` with a simple "
"example of a common setup: training a model with :obj:`mlx.nn.Module` using "
"an :obj:`mlx.optimizers.Optimizer` with state. We will show how to compile "
"the full forward, backward, and update with :func:`compile`."
msgstr ""

#: ../../../src/usage/compile.rst:298
msgid "To start, here is the simple example without any compilation:"
msgstr ""

#: ../../../src/usage/compile.rst:300
msgid ""
"import mlx.core as mx\n"
"import mlx.nn as nn\n"
"import mlx.optimizers as optim\n"
"\n"
"# 4 examples with 10 features each\n"
"x = mx.random.uniform(shape=(4, 10))\n"
"\n"
"# 0, 1 targets\n"
"y = mx.array([0, 1, 0, 1])\n"
"\n"
"# Simple linear model\n"
"model = nn.Linear(10, 1)\n"
"\n"
"# SGD with momentum\n"
"optimizer = optim.SGD(learning_rate=0.1, momentum=0.8)\n"
"\n"
"def loss_fn(model, x, y):\n"
"    logits = model(x).squeeze()\n"
"    return nn.losses.binary_cross_entropy(logits, y)\n"
"\n"
"loss_and_grad_fn = nn.value_and_grad(model, loss_fn)\n"
"\n"
"# Perform 10 steps of gradient descent\n"
"for it in range(10):\n"
"    loss, grads = loss_and_grad_fn(model, x, y)\n"
"    optimizer.update(model, grads)\n"
"    mx.eval(model.parameters(), optimizer.state)"
msgstr ""

#: ../../../src/usage/compile.rst:330
msgid ""
"To compile the update we can put it all in a function and compile it with "
"the appropriate input and output captures. Here's the same example but "
"compiled:"
msgstr ""

#: ../../../src/usage/compile.rst:333
msgid ""
"import mlx.core as mx\n"
"import mlx.nn as nn\n"
"import mlx.optimizers as optim\n"
"from functools import partial\n"
"\n"
"# 4 examples with 10 features each\n"
"x = mx.random.uniform(shape=(4, 10))\n"
"\n"
"# 0, 1 targets\n"
"y = mx.array([0, 1, 0, 1])\n"
"\n"
"# Simple linear model\n"
"model = nn.Linear(10, 1)\n"
"\n"
"# SGD with momentum\n"
"optimizer = optim.SGD(learning_rate=0.1, momentum=0.8)\n"
"\n"
"def loss_fn(model, x, y):\n"
"    logits = model(x).squeeze()\n"
"    return nn.losses.binary_cross_entropy(logits, y)\n"
"\n"
"# The state that will be captured as input and output\n"
"state = [model.state, optimizer.state]\n"
"\n"
"@partial(mx.compile, inputs=state, outputs=state)\n"
"def step(x, y):\n"
"    loss_and_grad_fn = nn.value_and_grad(model, loss_fn)\n"
"    loss, grads = loss_and_grad_fn(model, x, y)\n"
"    optimizer.update(model, grads)\n"
"    return loss\n"
"\n"
"# Perform 10 steps of gradient descent\n"
"for it in range(10):\n"
"    loss = step(x, y)\n"
"    # Evaluate the model and optimizer state\n"
"    mx.eval(state)\n"
"    print(loss)"
msgstr ""

#: ../../../src/usage/compile.rst:376
msgid ""
"If you are using a module which performs random sampling such as :func:`mlx."
"nn.Dropout`, make sure you also include ``mx.random.state`` in the ``state`` "
"captured by :func:`compile`, i.e. ``state = [model.state, optimizer.state, "
"mx.random.state]``."
msgstr ""

#: ../../../src/usage/compile.rst:384
msgid ""
"For more examples of compiling full training graphs checkout the  `MLX "
"Examples <https://github.com/ml-explore/mlx-examples>`_ GitHub repo."
msgstr ""

#: ../../../src/usage/compile.rst:388
msgid "Transformations with Compile"
msgstr ""

#: ../../../src/usage/compile.rst:390
msgid ""
"In MLX function transformations are composable. You can apply any function "
"transformation to the output of any other function transformation. For more "
"on this, see the documentation on :ref:`function transforms "
"<function_transforms>`."
msgstr ""

#: ../../../src/usage/compile.rst:395
msgid "Compiling transformed functions works just as expected:"
msgstr ""

#: ../../../src/usage/compile.rst:397
msgid ""
"grad_fn = mx.grad(mx.exp)\n"
"\n"
"compiled_grad_fn = mx.compile(grad_fn)\n"
"\n"
"# Prints: array(2.71828, dtype=float32)\n"
"print(grad_fn(mx.array(1.0)))\n"
"\n"
"# Also prints: array(2.71828, dtype=float32)\n"
"print(compiled_grad_fn(mx.array(1.0)))"
msgstr ""

#: ../../../src/usage/compile.rst:411
msgid ""
"In order to compile as much as possible, a transformation of a compiled "
"function will not by default be compiled. To compile the transformed "
"function simply pass it through :func:`compile`."
msgstr ""

#: ../../../src/usage/compile.rst:415
msgid ""
"You can also compile functions which themselves call compiled functions. A "
"good practice is to compile the outer most function to give :func:`compile` "
"the most opportunity to optimize the computation graph:"
msgstr ""

#: ../../../src/usage/compile.rst:419
msgid ""
"@mx.compile\n"
"def inner(x):\n"
"    return mx.exp(-mx.abs(x))\n"
"\n"
"def outer(x):\n"
"    inner(inner(x))\n"
"\n"
"# Compiling the outer function is good to do as it will likely\n"
"# be faster even though the inner functions are compiled\n"
"fun = mx.compile(outer)"
msgstr ""
