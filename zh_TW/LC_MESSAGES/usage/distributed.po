# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2023, MLX Contributors
# This file is distributed under the same license as the MLX package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: MLX 0.25\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-12-08 12:39+0000\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"Language: zh_TW\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"

#: ../../../src/usage/distributed.rst:4
msgid "Distributed Communication"
msgstr "分散式通訊"

#: ../../../src/usage/distributed.rst:8
msgid ""
"MLX supports distributed communication operations that allow the "
"computational cost of training or inference to be shared across many "
"physical machines. At the moment we support two different communication "
"backends:"
msgstr ""

#: ../../../src/usage/distributed.rst:12
msgid ""
"`MPI <https://en.wikipedia.org/wiki/Message_Passing_Interface>`_ a full-"
"featured and mature distributed communications library"
msgstr ""

#: ../../../src/usage/distributed.rst:14
msgid ""
"A **ring** backend of our own that uses native TCP sockets and should be "
"faster for thunderbolt connections."
msgstr ""

#: ../../../src/usage/distributed.rst:17
msgid ""
"The list of all currently supported operations and their documentation can "
"be seen in the :ref:`API docs<distributed>`."
msgstr "目前支援的所有操作及其文件，請參見 :ref:`API 文件<distributed>`。"

#: ../../../src/usage/distributed.rst:21
msgid ""
"Some operations may not be supported or not as fast as they should be. We "
"are adding more and tuning the ones we have as we are figuring out the best "
"way to do distributed computing on Macs using MLX."
msgstr ""

#: ../../../src/usage/distributed.rst:26
msgid "Getting Started"
msgstr "開始使用"

#: ../../../src/usage/distributed.rst:28
msgid "A distributed program in MLX is as simple as:"
msgstr "在 MLX 中，分散式程式可簡單寫成："

#: ../../../src/usage/distributed.rst:30
msgid ""
"import mlx.core as mx\n"
"\n"
"world = mx.distributed.init()\n"
"x = mx.distributed.all_sum(mx.ones(10))\n"
"print(world.rank(), x)"
msgstr ""

#: ../../../src/usage/distributed.rst:38
msgid ""
"The program above sums the array ``mx.ones(10)`` across all distributed "
"processes. However, when this script is run with ``python`` only one process "
"is launched and no distributed communication takes place. Namely, all "
"operations in ``mx.distributed`` are noops when the distributed group has a "
"size of one. This property allows us to avoid code that checks if we are in "
"a distributed setting similar to the one below:"
msgstr ""
"上述程式會在所有分散式行程之間對 ``mx.ones(10)`` 進行求和。然而用 ``python`` "
"執行此腳本時只會啟動一個行程，因此不會進行分散式通訊。也就是說，當分散式群組"
"大小為 1 時，``mx.distributed`` 中的所有操作都是空操作。這個特性讓我們可以避"
"免寫出需要檢查是否在分散式環境下的程式碼，例如："

#: ../../../src/usage/distributed.rst:45
msgid ""
"import mlx.core as mx\n"
"\n"
"x = ...\n"
"world = mx.distributed.init()\n"
"# No need for the check we can simply do x = mx.distributed.all_sum(x)\n"
"if world.size() > 1:\n"
"    x = mx.distributed.all_sum(x)"
msgstr ""

#: ../../../src/usage/distributed.rst:56
msgid "Running Distributed Programs"
msgstr "執行分散式程式"

#: ../../../src/usage/distributed.rst:58
msgid ""
"MLX provides ``mlx.launch`` a helper script to launch distributed programs. "
"Continuing with our initial example we can run it on localhost with 4 "
"processes using"
msgstr ""
"MLX 提供 ``mlx.launch`` 這個輔助腳本用來啟動分散式程式。延續前面的範例，我們"
"可以在本機以 4 個行程執行："

#: ../../../src/usage/distributed.rst:61
msgid ""
"$ mlx.launch -n 4 my_script.py\n"
"3 array([4, 4, 4, ..., 4, 4, 4], dtype=float32)\n"
"2 array([4, 4, 4, ..., 4, 4, 4], dtype=float32)\n"
"1 array([4, 4, 4, ..., 4, 4, 4], dtype=float32)\n"
"0 array([4, 4, 4, ..., 4, 4, 4], dtype=float32)"
msgstr ""

#: ../../../src/usage/distributed.rst:69
msgid ""
"We can also run it on some remote hosts by providing their IPs (provided "
"that the script exists on all hosts and they are reachable by ssh)"
msgstr ""
"我們也可以提供遠端主機的 IP 來執行（前提是腳本在所有主機上都存在，且可透過 "
"ssh 連線）。"

#: ../../../src/usage/distributed.rst:72
msgid ""
"$ mlx.launch --hosts ip1,ip2,ip3,ip4 my_script.py\n"
"3 array([4, 4, 4, ..., 4, 4, 4], dtype=float32)\n"
"2 array([4, 4, 4, ..., 4, 4, 4], dtype=float32)\n"
"1 array([4, 4, 4, ..., 4, 4, 4], dtype=float32)\n"
"0 array([4, 4, 4, ..., 4, 4, 4], dtype=float32)"
msgstr ""

#: ../../../src/usage/distributed.rst:80
msgid ""
"Consult the dedicated :doc:`usage guide<launching_distributed>` for more "
"information on using ``mlx.launch``."
msgstr ""
"更多關於 ``mlx.launch`` 的使用方式，請參考專門的 :doc:`使用指南"
"<launching_distributed>`。"

#: ../../../src/usage/distributed.rst:84
msgid "Selecting Backend"
msgstr "選擇後端"

#: ../../../src/usage/distributed.rst:86
msgid ""
"You can select the backend you want to use when calling :func:`init` by "
"passing one of ``{'any', 'ring', 'mpi'}``. When passing ``any``, MLX will "
"try to initialize the ``ring`` backend and if it fails the ``mpi`` backend. "
"If they both fail then a singleton group is created."
msgstr ""

#: ../../../src/usage/distributed.rst:92
msgid ""
"After a distributed backend is successfully initialized :func:`init` will "
"return **the same backend** if called without arguments or with backend set "
"to ``any``."
msgstr ""
"分散式後端成功初始化後，再次呼叫 :func:`init`（不帶參數或設定為 ``any``）會回"
"傳**相同的後端**。"

#: ../../../src/usage/distributed.rst:96
msgid ""
"The following examples aim to clarify the backend initialization logic in "
"MLX:"
msgstr "以下範例用來說明 MLX 中後端初始化的邏輯："

#: ../../../src/usage/distributed.rst:98
msgid ""
"# Case 1: Initialize MPI regardless if it was possible to initialize the "
"ring backend\n"
"world = mx.distributed.init(backend=\"mpi\")\n"
"world2 = mx.distributed.init()  # subsequent calls return the MPI backend!\n"
"\n"
"# Case 2: Initialize any backend\n"
"world = mx.distributed.init(backend=\"any\")  # equivalent to no arguments\n"
"world2 = mx.distributed.init()  # same as above\n"
"\n"
"# Case 3: Initialize both backends at the same time\n"
"world_mpi = mx.distributed.init(backend=\"mpi\")\n"
"world_ring = mx.distributed.init(backend=\"ring\")\n"
"world_any = mx.distributed.init()  # same as MPI because it was initialized "
"first!"
msgstr ""

#: ../../../src/usage/distributed.rst:114
msgid "Training Example"
msgstr "訓練範例"

#: ../../../src/usage/distributed.rst:116
msgid ""
"In this section we will adapt an MLX training loop to support data parallel "
"distributed training. Namely, we will average the gradients across a set of "
"hosts before applying them to the model."
msgstr ""
"本節將把 MLX 的訓練迴圈改為支援資料平行的分散式訓練。也就是在將梯度套用到模型"
"前，先在多台主機間對梯度取平均。"

#: ../../../src/usage/distributed.rst:120
msgid ""
"Our training loop looks like the following code snippet if we omit the "
"model, dataset and optimizer initialization."
msgstr "若省略模型、資料集與最佳化器的初始化，我們的訓練迴圈如下："

#: ../../../src/usage/distributed.rst:123
msgid ""
"model = ...\n"
"optimizer = ...\n"
"dataset = ...\n"
"\n"
"def step(model, x, y):\n"
"    loss, grads = loss_grad_fn(model, x, y)\n"
"    optimizer.update(model, grads)\n"
"    return loss\n"
"\n"
"for x, y in dataset:\n"
"    loss = step(model, x, y)\n"
"    mx.eval(loss, model.parameters())"
msgstr ""

#: ../../../src/usage/distributed.rst:138
msgid ""
"All we have to do to average the gradients across machines is perform an :"
"func:`all_sum` and divide by the size of the :class:`Group`. Namely we have "
"to :func:`mlx.utils.tree_map` the gradients with following function."
msgstr ""
"要在多台機器間對梯度取平均，只需執行 :func:`all_sum` 並除以 :class:`Group` 的"
"大小。也就是用下列函式對梯度做 :func:`mlx.utils.tree_map`："

#: ../../../src/usage/distributed.rst:142
msgid ""
"def all_avg(x):\n"
"    return mx.distributed.all_sum(x) / mx.distributed.init().size()"
msgstr ""

#: ../../../src/usage/distributed.rst:147
msgid ""
"Putting everything together our training loop step looks as follows with "
"everything else remaining the same."
msgstr "把所有內容整合後，在其他部分不變的情況下，訓練步驟如下："

#: ../../../src/usage/distributed.rst:150
msgid ""
"from mlx.utils import tree_map\n"
"\n"
"def all_reduce_grads(grads):\n"
"    N = mx.distributed.init().size()\n"
"    if N == 1:\n"
"        return grads\n"
"    return tree_map(\n"
"        lambda x: mx.distributed.all_sum(x) / N,\n"
"        grads\n"
"    )\n"
"\n"
"def step(model, x, y):\n"
"    loss, grads = loss_grad_fn(model, x, y)\n"
"    grads = all_reduce_grads(grads)  # <--- This line was added\n"
"    optimizer.update(model, grads)\n"
"    return loss"
msgstr ""

#: ../../../src/usage/distributed.rst:170
msgid "Utilizing ``nn.average_gradients``"
msgstr "使用 ``nn.average_gradients``"

#: ../../../src/usage/distributed.rst:172
msgid ""
"Although the code example above works correctly; it performs one "
"communication per gradient. It is significantly more efficient to aggregate "
"several gradients together and perform fewer communication steps."
msgstr ""
"雖然上面的程式碼可以正確運作，但每個梯度都會進行一次通訊。將多個梯度聚合後再"
"通訊，會更有效率。"

#: ../../../src/usage/distributed.rst:176
msgid ""
"This is the purpose of :func:`mlx.nn.average_gradients`. The final code "
"looks almost identical to the example above:"
msgstr ""
"這正是 :func:`mlx.nn.average_gradients` 的目的。最終程式碼與上例幾乎相同："

#: ../../../src/usage/distributed.rst:179
msgid ""
"model = ...\n"
"optimizer = ...\n"
"dataset = ...\n"
"\n"
"def step(model, x, y):\n"
"    loss, grads = loss_grad_fn(model, x, y)\n"
"    grads = mlx.nn.average_gradients(grads) # <---- This line was added\n"
"    optimizer.update(model, grads)\n"
"    return loss\n"
"\n"
"for x, y in dataset:\n"
"    loss = step(model, x, y)\n"
"    mx.eval(loss, model.parameters())"
msgstr ""

#: ../../../src/usage/distributed.rst:197
msgid "Getting Started with MPI"
msgstr "開始使用 MPI"

#: ../../../src/usage/distributed.rst:199
msgid ""
"MLX already comes with the ability to \"talk\" to MPI if it is installed on "
"the machine. Launching distributed MLX programs that use MPI can be done "
"with ``mpirun`` as expected. However, in the following examples we will be "
"using ``mlx.launch --backend mpi`` which takes care of some nuisances such "
"as setting absolute paths for the ``mpirun`` executable and the ``libmpi."
"dyld`` shared library."
msgstr ""

#: ../../../src/usage/distributed.rst:206
msgid ""
"The simplest possible usage is the following which, assuming the minimal "
"example in the beginning of this page, should result in:"
msgstr "最簡單的用法如下，假設使用本頁開頭的最小範例，輸出應為："

#: ../../../src/usage/distributed.rst:209
msgid ""
"$ mlx.launch --backend mpi -n 2 test.py\n"
"1 array([2, 2, 2, ..., 2, 2, 2], dtype=float32)\n"
"0 array([2, 2, 2, ..., 2, 2, 2], dtype=float32)"
msgstr ""

#: ../../../src/usage/distributed.rst:215
msgid ""
"The above launches two processes on the same (local) machine and we can see "
"both standard output streams. The processes send the array of 1s to each "
"other and compute the sum which is printed. Launching with ``mlx.launch -n "
"4 ...`` would print 4 etc."
msgstr ""
"以上會在同一台（本地）機器上啟動兩個行程，並可看到兩個標準輸出。行程會互相傳"
"送全 1 陣列並計算總和後印出。若使用 ``mlx.launch -n 4 ...`` 則會印出 4，依此"
"類推。"

#: ../../../src/usage/distributed.rst:221
msgid "Installing MPI"
msgstr "安裝 MPI"

#: ../../../src/usage/distributed.rst:223
msgid ""
"MPI can be installed with Homebrew, using the Anaconda package manager or "
"compiled from source. Most of our testing is done using ``openmpi`` "
"installed with the Anaconda package manager as follows:"
msgstr ""

#: ../../../src/usage/distributed.rst:227
msgid "$ conda install conda-forge::openmpi"
msgstr ""

#: ../../../src/usage/distributed.rst:231
msgid ""
"Installing with Homebrew may require specifying the location of ``libmpi."
"dyld`` so that MLX can find it and load it at runtime. This can simply be "
"achieved by passing the ``DYLD_LIBRARY_PATH`` environment variable to "
"``mpirun`` and it is done automatically by ``mlx.launch``."
msgstr ""

#: ../../../src/usage/distributed.rst:236
msgid ""
"$ mpirun -np 2 -x DYLD_LIBRARY_PATH=/opt/homebrew/lib/ python test.py\n"
"$ # or simply\n"
"$ mlx.launch -n 2 test.py"
msgstr ""

#: ../../../src/usage/distributed.rst:243
msgid "Setting up Remote Hosts"
msgstr "設定遠端主機"

#: ../../../src/usage/distributed.rst:245
msgid ""
"MPI can automatically connect to remote hosts and set up the communication "
"over the network if the remote hosts can be accessed via ssh. A good "
"checklist to debug connectivity issues is the following:"
msgstr ""
"若可透過 ssh 連線到遠端主機，MPI 便可自動連線並建立網路通訊。以下是排查連線問"
"題的檢查列表："

#: ../../../src/usage/distributed.rst:249
msgid ""
"``ssh hostname`` works from all machines to all machines without asking for "
"password or host confirmation"
msgstr "所有機器之間執行 ``ssh hostname`` 不需密碼或主機確認"

#: ../../../src/usage/distributed.rst:251
msgid "``mpirun`` is accessible on all machines."
msgstr "所有機器皆可存取 ``mpirun``。"

#: ../../../src/usage/distributed.rst:252
msgid ""
"Ensure that the ``hostname`` used by MPI is the one that you have configured "
"in the ``.ssh/config`` files on all machines."
msgstr ""
"請確保 MPI 使用的 ``hostname`` 與所有機器 ``.ssh/config`` 中設定的一致。"

#: ../../../src/usage/distributed.rst:256
msgid "Tuning MPI All Reduce"
msgstr "調校 MPI All Reduce"

#: ../../../src/usage/distributed.rst:260
msgid ""
"For faster all reduce consider using the ring backend either with "
"Thunderbolt connections or over Ethernet."
msgstr ""
"若要更快的 all-reduce，可考慮使用 ring 後端（透過 Thunderbolt 連線或乙太網"
"路）。"

#: ../../../src/usage/distributed.rst:263
msgid ""
"Configure MPI to use N tcp connections between each host to improve "
"bandwidth by passing ``--mca btl_tcp_links N``."
msgstr ""
"透過 ``--mca btl_tcp_links N`` 設定 MPI 在每個主機間使用 N 條 TCP 連線，以提"
"升頻寬。"

#: ../../../src/usage/distributed.rst:266
msgid ""
"Force MPI to use the most performant network interface by setting ``--mca "
"btl_tcp_if_include <iface>`` where ``<iface>`` should be the interface you "
"want to use."
msgstr ""
"透過 ``--mca btl_tcp_if_include <iface>`` 強制 MPI 使用效能最佳的網路介面，其"
"中 ``<iface>`` 為你想使用的介面。"

#: ../../../src/usage/distributed.rst:271
msgid "Getting Started with Ring"
msgstr "Ring 入門"

#: ../../../src/usage/distributed.rst:273
msgid ""
"The ring backend does not depend on any third party library so it is always "
"available. It uses TCP sockets so the nodes need to be reachable via a "
"network. As the name suggests the nodes are connected in a ring which means "
"that rank 1 can only communicate with rank 0 and rank 2, rank 2 only with "
"rank 1 and rank 3 and so on and so forth. As a result :func:`send` and :func:"
"`recv` with arbitrary sender and receiver is not supported in the ring "
"backend."
msgstr ""

#: ../../../src/usage/distributed.rst:281
msgid "Defining a Ring"
msgstr "定義 Ring"

#: ../../../src/usage/distributed.rst:283
msgid ""
"The easiest way to define and use a ring is via a JSON hostfile and the "
"``mlx.launch`` :doc:`helper script <launching_distributed>`. For each node "
"one defines a hostname to ssh into to run commands on this node and one or "
"more IPs that this node will listen to for connections."
msgstr ""
"定義並使用 ring 最簡單的方法，是透過 JSON hostfile 與 ``mlx.launch`` :doc:`輔"
"助腳本 <launching_distributed>`。對每個節點，需定義可 ssh 登入的主機名稱以及"
"一個或多個供其監聽連線的 IP。"

#: ../../../src/usage/distributed.rst:288
msgid ""
"For example the hostfile below defines a 4 node ring. ``hostname1`` will be "
"rank 0, ``hostname2`` rank 1 etc."
msgstr ""
"例如，以下 hostfile 定義了 4 節點的 ring。``hostname1`` 為 rank 0，"
"``hostname2`` 為 rank 1，以此類推。"

#: ../../../src/usage/distributed.rst:291
msgid ""
"[\n"
"    {\"ssh\": \"hostname1\", \"ips\": [\"123.123.123.1\"]},\n"
"    {\"ssh\": \"hostname2\", \"ips\": [\"123.123.123.2\"]},\n"
"    {\"ssh\": \"hostname3\", \"ips\": [\"123.123.123.3\"]},\n"
"    {\"ssh\": \"hostname4\", \"ips\": [\"123.123.123.4\"]}\n"
"]"
msgstr ""

#: ../../../src/usage/distributed.rst:300
msgid ""
"Running ``mlx.launch --hostfile ring-4.json my_script.py`` will ssh into "
"each node, run the script which will listen for connections in each of the "
"provided IPs. Specifically, ``hostname1`` will connect to ``123.123.123.2`` "
"and accept a connection from ``123.123.123.4`` and so on and so forth."
msgstr ""
"執行 ``mlx.launch --hostfile ring-4.json my_script.py`` 會 ssh 進入每個節點，"
"並執行腳本、在提供的 IP 上監聽連線。具體來說，``hostname1`` 會連線到 "
"``123.123.123.2`` 並接受來自 ``123.123.123.4`` 的連線，以此類推。"

#: ../../../src/usage/distributed.rst:306
msgid "Thunderbolt Ring"
msgstr ""

#: ../../../src/usage/distributed.rst:308
msgid ""
"Although the ring backend can have benefits over MPI even for Ethernet, its "
"main purpose is to use Thunderbolt rings for higher bandwidth communication. "
"Setting up such thunderbolt rings can be done manually, but is a relatively "
"tedious process. To simplify this, we provide the utility ``mlx."
"distributed_config``."
msgstr ""
"即使在乙太網路上，ring 後端也可能優於 MPI，但其主要目的在於使用 Thunderbolt "
"ring 來取得更高頻寬通訊。此類 Thunderbolt ring 可以手動設定，但流程相對繁瑣。"
"為了簡化，我們提供 ``mlx.distributed_config`` 工具。"

#: ../../../src/usage/distributed.rst:313
msgid ""
"To use ``mlx.distributed_config`` your computers need to be accessible by "
"ssh via Ethernet or Wi-Fi. Subsequently, connect them via thunderbolt cables "
"and then call the utility as follows:"
msgstr ""
"要使用 ``mlx.distributed_config``，你的電腦需能透過乙太網路或 Wi-Fi 以 ssh 連"
"線。接著用 Thunderbolt 線纜連接它們，並如下呼叫工具："

#: ../../../src/usage/distributed.rst:317
msgid "mlx.distributed_config --verbose --hosts host1,host2,host3,host4"
msgstr ""

#: ../../../src/usage/distributed.rst:321
msgid ""
"By default the script will attempt to discover the thunderbolt ring and "
"provide you with the commands to configure each node as well as the "
"``hostfile.json`` to use with ``mlx.launch``. If password-less ``sudo`` is "
"available on the nodes then ``--auto-setup`` can be used to configure them "
"automatically."
msgstr ""
"預設情況下，腳本會嘗試偵測 Thunderbolt ring，並提供設定各節點的命令，以及可"
"供 ``mlx.launch`` 使用的 ``hostfile.json``。若各節點可無密碼 ``sudo``，可使"
"用 ``--auto-setup`` 自動完成設定。"

#: ../../../src/usage/distributed.rst:326
msgid ""
"To validate your connection without configuring anything ``mlx."
"distributed_config`` can also plot the ring using DOT format."
msgstr ""

#: ../../../src/usage/distributed.rst:329
msgid ""
"mlx.distributed_config --verbose --hosts host1,host2,host3,host4 --dot >ring."
"dot\n"
"dot -Tpng ring.dot >ring.png\n"
"open ring.png"
msgstr ""

#: ../../../src/usage/distributed.rst:335
msgid ""
"If you want to go through the process manually, the steps are as follows:"
msgstr "如果想手動完成，步驟如下："

#: ../../../src/usage/distributed.rst:337
msgid "Disable the thunderbolt bridge interface"
msgstr "停用 Thunderbolt bridge 介面"

#: ../../../src/usage/distributed.rst:338
msgid ""
"For the cable connecting rank ``i`` to rank ``i + 1`` find the interfaces "
"corresponding to that cable in nodes ``i`` and ``i + 1``."
msgstr ""
"針對連接 rank ``i`` 與 rank ``i + 1`` 的線纜，在節點 ``i`` 與 ``i + 1`` 上找"
"出對應的介面。"

#: ../../../src/usage/distributed.rst:340
msgid ""
"Set up a unique subnetwork connecting the two nodes for the corresponding "
"interfaces. For instance if the cable corresponds to ``en2`` on node ``i`` "
"and ``en2`` also on node ``i + 1`` then we may assign IPs ``192.168.0.1`` "
"and ``192.168.0.2`` respectively to the two nodes. For more details you can "
"see the commands prepared by the utility script."
msgstr ""
"為對應介面建立一個專用子網路以連接兩個節點。例如該線纜在節點 ``i`` 對應 "
"``en2``，且在節點 ``i + 1`` 也對應 ``en2``，則可分別配置 IP ``192.168.0.1`` "
"與 ``192.168.0.2``。更多細節可參考工具腳本準備的命令。"
