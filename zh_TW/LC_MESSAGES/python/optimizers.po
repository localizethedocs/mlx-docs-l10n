# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2023, MLX Contributors
# This file is distributed under the same license as the MLX package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: MLX 0.22\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-12-08 12:45+0000\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"Language: zh_TW\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"

#: ../../../src/python/optimizers.rst:6
msgid "Optimizers"
msgstr "最佳化器"

#: ../../../src/python/optimizers.rst:8
msgid ""
"The optimizers in MLX can be used both with :mod:`mlx.nn` but also with "
"pure :mod:`mlx.core` functions. A typical example involves calling :meth:"
"`Optimizer.update` to update a model's parameters based on the loss "
"gradients and subsequently calling :func:`mlx.core.eval` to evaluate both "
"the model's parameters and the **optimizer state**."
msgstr ""
"MLX 的最佳化器既可與 :mod:`mlx.nn` 搭配使用，也可用於純 :mod:`mlx.core` 函"
"式。典型作法是呼叫 :meth:`Optimizer.update` 依據損失梯度更新模型參數，接著呼"
"叫 :func:`mlx.core.eval` 同時計算模型參數與 **最佳化器狀態**。"

#: ../../../src/python/optimizers.rst:14
msgid ""
"# Create a model\n"
"model = MLP(num_layers, train_images.shape[-1], hidden_dim, num_classes)\n"
"mx.eval(model.parameters())\n"
"\n"
"# Create the gradient function and the optimizer\n"
"loss_and_grad_fn = nn.value_and_grad(model, loss_fn)\n"
"optimizer = optim.SGD(learning_rate=learning_rate)\n"
"\n"
"for e in range(num_epochs):\n"
"    for X, y in batch_iterate(batch_size, train_images, train_labels):\n"
"        loss, grads = loss_and_grad_fn(model, X, y)\n"
"\n"
"        # Update the model with the gradients. So far no computation has "
"happened.\n"
"        optimizer.update(model, grads)\n"
"\n"
"        # Compute the new parameters but also the optimizer state.\n"
"        mx.eval(model.parameters(), optimizer.state)"
msgstr ""

#: ../../../src/python/optimizers.rst:35
msgid "Saving and Loading"
msgstr "儲存與載入"

#: ../../../src/python/optimizers.rst:37
msgid ""
"To serialize an optimizer, save its state. To load an optimizer, load and "
"set the saved state. Here's a simple example:"
msgstr ""
"若要序列化最佳化器，請儲存其狀態。要載入最佳化器，則載入並設定已儲存的狀態。"
"以下是簡單範例："

#: ../../../src/python/optimizers.rst:40
msgid ""
"import mlx.core as mx\n"
"from mlx.utils import tree_flatten, tree_unflatten\n"
"import mlx.optimizers as optim\n"
"\n"
"optimizer = optim.Adam(learning_rate=1e-2)\n"
"\n"
"# Perform some updates with the optimizer\n"
"model = {\"w\" : mx.zeros((5, 5))}\n"
"grads = {\"w\" : mx.ones((5, 5))}\n"
"optimizer.update(model, grads)\n"
"\n"
"# Save the state\n"
"state = tree_flatten(optimizer.state)\n"
"mx.save_safetensors(\"optimizer.safetensors\", dict(state))\n"
"\n"
"# Later on, for example when loading from a checkpoint,\n"
"# recreate the optimizer and load the state\n"
"optimizer = optim.Adam(learning_rate=1e-2)\n"
"\n"
"state = tree_unflatten(list(mx.load(\"optimizer.safetensors\").items()))\n"
"optimizer.state = state"
msgstr ""

#: ../../../src/python/optimizers.rst:64
msgid ""
"Note, not every optimizer configuation parameter is saved in the state. For "
"example, for Adam the learning rate is saved but the ``betas`` and ``eps`` "
"parameters are not. A good rule of thumb is if the parameter can be "
"scheduled then it will be included in the optimizer state."
msgstr ""
"注意，並非每個最佳化器的設定參數都會儲存在狀態中。例如對 Adam 而言，學習率會"
"被儲存，但 ``betas`` 與 ``eps`` 參數不會。經驗法則是：若該參數可被排程，則會"
"包含在最佳化器狀態中。"

#: ../../../src/python/optimizers.rst:78:<autosummary>:1
msgid ""
":py:obj:`clip_grad_norm <mlx.optimizers.clip_grad_norm>`\\ \\(grads\\, "
"max\\_norm\\)"
msgstr ""

#: ../../../src/python/optimizers.rst:78:<autosummary>:1
msgid "Clips the global norm of the gradients."
msgstr "裁剪梯度的全域範數。"
