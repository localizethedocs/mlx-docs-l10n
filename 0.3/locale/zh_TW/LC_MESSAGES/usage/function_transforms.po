# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2023, MLX Contributors
# This file is distributed under the same license as the MLX package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: MLX 0.3\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-12-08 12:56+0000\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"Language: zh_TW\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"

#: ../../../src/usage/function_transforms.rst:4
msgid "Function Transforms"
msgstr "函式轉換"

#: ../../../src/usage/function_transforms.rst:8
msgid ""
"MLX uses composable function transformations for automatic differentiation, "
"vectorization, and compute graph optimizations. To see the complete list of "
"function transformations check-out the :ref:`API documentation <transforms>`."
msgstr ""
"MLX 使用可組合的函式轉換來進行自動微分、向量化與計算圖最佳化。要查看完整的函"
"式轉換清單，請參考 :ref:`API 文件 <transforms>`。"

#: ../../../src/usage/function_transforms.rst:12
msgid ""
"The key idea behind composable function transformations is that every "
"transformation returns a function which can be further transformed."
msgstr ""
"可組合函式轉換的關鍵概念是：每個轉換都會回傳一個可再被進一步轉換的函式。"

#: ../../../src/usage/function_transforms.rst:15
msgid "Here is a simple example:"
msgstr "以下是一個簡單範例："

#: ../../../src/usage/function_transforms.rst:17
msgid ""
">>> dfdx = mx.grad(mx.sin)\n"
">>> dfdx(mx.array(mx.pi))\n"
"array(-1, dtype=float32)\n"
">>> mx.cos(mx.array(mx.pi))\n"
"array(-1, dtype=float32)"
msgstr ""

#: ../../../src/usage/function_transforms.rst:26
msgid ""
"The output of :func:`grad` on :func:`sin` is simply another function. In "
"this case it is the gradient of the sine function which is exactly the "
"cosine function. To get the second derivative you can do:"
msgstr ""
":func:`grad` 作用於 :func:`sin` 的輸出本身就是另一個函式。在此情況下，它是正"
"弦函式的梯度，也就是餘弦函式。若要取得二階導數，可以這樣做："

#: ../../../src/usage/function_transforms.rst:30
msgid ""
">>> d2fdx2 = mx.grad(mx.grad(mx.sin))\n"
">>> d2fdx2(mx.array(mx.pi / 2))\n"
"array(-1, dtype=float32)\n"
">>> mx.sin(mx.array(mx.pi / 2))\n"
"array(1, dtype=float32)"
msgstr ""

#: ../../../src/usage/function_transforms.rst:38
msgid ""
"Using :func:`grad` on the output of :func:`grad` is always ok. You keep "
"getting higher order derivatives."
msgstr ""
"對 :func:`grad` 的輸出再使用 :func:`grad` 一直都是可行的，會持續得到更高階導"
"數。"

#: ../../../src/usage/function_transforms.rst:41
msgid ""
"Any of the MLX function transformations can be composed in any order to any "
"depth. See the following sections for more information on :ref:`automatic "
"differentiaion <auto diff>` and :ref:`automatic vectorization <vmap>`. For "
"more information on :func:`compile` see the :ref:`compile documentation "
"<compile>`."
msgstr ""

#: ../../../src/usage/function_transforms.rst:48
msgid "Automatic Differentiation"
msgstr "自動微分"

#: ../../../src/usage/function_transforms.rst:52
msgid ""
"Automatic differentiation in MLX works on functions rather than on implicit "
"graphs."
msgstr "MLX 的自動微分是針對函式運作，而非隱式計算圖。"

#: ../../../src/usage/function_transforms.rst:57
msgid ""
"If you are coming to MLX from PyTorch, you no longer need functions like "
"``backward``, ``zero_grad``, and ``detach``, or properties like "
"``requires_grad``."
msgstr ""
"如果你是從 PyTorch 轉到 MLX，就不再需要像 ``backward``、``zero_grad``、"
"``detach`` 這類函式，或是 ``requires_grad`` 這類屬性。"

#: ../../../src/usage/function_transforms.rst:61
msgid ""
"The most basic example is taking the gradient of a scalar-valued function as "
"we saw above. You can use the :func:`grad` and :func:`value_and_grad` "
"function to compute gradients of more complex functions. By default these "
"functions compute the gradient with respect to the first argument:"
msgstr ""
"最基本的例子就是像上面那樣對標量值函式取梯度。你可以使用 :func:`grad` 與 :"
"func:`value_and_grad` 來計算更複雜函式的梯度。預設情況下，這些函式會對第一個"
"引數計算梯度："

#: ../../../src/usage/function_transforms.rst:66
msgid ""
"def loss_fn(w, x, y):\n"
"   return mx.mean(mx.square(w * x - y))\n"
"\n"
"w = mx.array(1.0)\n"
"x = mx.array([0.5, -0.5])\n"
"y = mx.array([1.5, -1.5])\n"
"\n"
"# Computes the gradient of loss_fn with respect to w:\n"
"grad_fn = mx.grad(loss_fn)\n"
"dloss_dw = grad_fn(w, x, y)\n"
"# Prints array(-1, dtype=float32)\n"
"print(dloss_dw)\n"
"\n"
"# To get the gradient with respect to x we can do:\n"
"grad_fn = mx.grad(loss_fn, argnums=1)\n"
"dloss_dx = grad_fn(w, x, y)\n"
"# Prints array([-1, 1], dtype=float32)\n"
"print(dloss_dx)"
msgstr ""

#: ../../../src/usage/function_transforms.rst:88
msgid ""
"One way to get the loss and gradient is to call ``loss_fn`` followed by "
"``grad_fn``, but this can result in a lot of redundant work. Instead, you "
"should use :func:`value_and_grad`. Continuing the above example:"
msgstr ""
"取得損失與梯度的一種方式是先呼叫 ``loss_fn`` 再呼叫 ``grad_fn``，但這可能造成"
"大量重複計算。相反地，你應該使用 :func:`value_and_grad`。沿用上例："

#: ../../../src/usage/function_transforms.rst:93
msgid ""
"# Computes the gradient of loss_fn with respect to w:\n"
"loss_and_grad_fn = mx.value_and_grad(loss_fn)\n"
"loss, dloss_dw = loss_and_grad_fn(w, x, y)\n"
"\n"
"# Prints array(1, dtype=float32)\n"
"print(loss)\n"
"\n"
"# Prints array(-1, dtype=float32)\n"
"print(dloss_dw)"
msgstr ""

#: ../../../src/usage/function_transforms.rst:106
msgid ""
"You can also take the gradient with respect to arbitrarily nested Python "
"containers of arrays (specifically any of :obj:`list`, :obj:`tuple`, or :obj:"
"`dict`)."
msgstr ""
"你也可以對任意巢狀的 Python 容器中的陣列取梯度（具體來說是 :obj:`list`、:obj:"
"`tuple` 或 :obj:`dict`）。"

#: ../../../src/usage/function_transforms.rst:110
msgid ""
"Suppose we wanted a weight and a bias parameter in the above example. A nice "
"way to do that is the following:"
msgstr "假設在上例中我們想加入權重與偏置參數，一種不錯的寫法如下："

#: ../../../src/usage/function_transforms.rst:113
msgid ""
"def loss_fn(params, x, y):\n"
"   w, b = params[\"weight\"], params[\"bias\"]\n"
"   h = w * x + b\n"
"   return mx.mean(mx.square(h - y))\n"
"\n"
"params = {\"weight\": mx.array(1.0), \"bias\": mx.array(0.0)}\n"
"x = mx.array([0.5, -0.5])\n"
"y = mx.array([1.5, -1.5])\n"
"\n"
"# Computes the gradient of loss_fn with respect to both the\n"
"# weight and bias:\n"
"grad_fn = mx.grad(loss_fn)\n"
"grads = grad_fn(params, x, y)\n"
"\n"
"# Prints\n"
"# {'weight': array(-1, dtype=float32), 'bias': array(0, dtype=float32)}\n"
"print(grads)"
msgstr ""

#: ../../../src/usage/function_transforms.rst:133
msgid ""
"Notice the tree structure of the parameters is preserved in the gradients."
msgstr "請注意參數的樹狀結構會在梯度中被保留。"

#: ../../../src/usage/function_transforms.rst:135
msgid ""
"In some cases you may want to stop gradients from propagating through a part "
"of the function. You can use the :func:`stop_gradient` for that."
msgstr ""
"在某些情況下，你可能希望梯度不要穿過函式的某一部分，此時可以使用 :func:"
"`stop_gradient`。"

#: ../../../src/usage/function_transforms.rst:140
msgid "Automatic Vectorization"
msgstr "自動向量化"

#: ../../../src/usage/function_transforms.rst:144
msgid ""
"Use :func:`vmap` to automate vectorizing complex functions. Here we'll go "
"through a basic and contrived example for the sake of clarity, but :func:"
"`vmap` can be quite powerful for more complex functions which are difficult "
"to optimize by hand."
msgstr ""
"使用 :func:`vmap` 來自動向量化複雜函式。為了清楚起見，這裡會用一個基本且刻意"
"簡化的例子，但對於難以手動最佳化的複雜函式，:func:`vmap` 其實非常強大。"

#: ../../../src/usage/function_transforms.rst:151
msgid ""
"Some operations are not yet supported with :func:`vmap`. If you encounter an "
"error like: ``ValueError: Primitive's vmap not implemented.`` file an `issue "
"<https://github.com/ml-explore/mlx/issues>`_ and include your function. We "
"will prioritize including it."
msgstr ""
"有些運算尚未支援 :func:`vmap`。若你遇到像 ``ValueError: Primitive's vmap not "
"implemented.`` 這樣的錯誤，請在 `issue <https://github.com/ml-explore/mlx/"
"issues>`_ 中回報並附上你的函式，我們會優先納入支援。"

#: ../../../src/usage/function_transforms.rst:156
msgid ""
"A naive way to add the elements from two sets of vectors is with a loop:"
msgstr "將兩組向量逐一相加的一種樸素作法是用迴圈："

#: ../../../src/usage/function_transforms.rst:158
msgid ""
"xs = mx.random.uniform(shape=(4096, 100))\n"
"ys = mx.random.uniform(shape=(100, 4096))\n"
"\n"
"def naive_add(xs, ys):\n"
"    return [xs[i] + ys[:, i] for i in range(xs.shape[1])]"
msgstr ""

#: ../../../src/usage/function_transforms.rst:166
msgid ""
"Instead you can use :func:`vmap` to automatically vectorize the addition:"
msgstr "相對地，你可以使用 :func:`vmap` 自動向量化相加："

#: ../../../src/usage/function_transforms.rst:168
msgid ""
"# Vectorize over the second dimension of x and the\n"
"# first dimension of y\n"
"vmap_add = mx.vmap(lambda x, y: x + y, in_axes=(1, 0))"
msgstr ""

#: ../../../src/usage/function_transforms.rst:174
msgid ""
"The ``in_axes`` parameter can be used to specify which dimensions of the "
"corresponding input to vectorize over. Similarly, use ``out_axes`` to "
"specify where the vectorized axes should be in the outputs."
msgstr ""
"``in_axes`` 參數可用來指定要對應輸入的哪些維度進行向量化。同樣地，使用 "
"``out_axes`` 來指定輸出中向量化軸的位置。"

#: ../../../src/usage/function_transforms.rst:178
msgid "Let's time these two different versions:"
msgstr "讓我們測量這兩種版本的耗時："

#: ../../../src/usage/function_transforms.rst:180
msgid ""
"import timeit\n"
"\n"
"print(timeit.timeit(lambda: mx.eval(naive_add(xs, ys)), number=100))\n"
"print(timeit.timeit(lambda: mx.eval(vmap_add(xs, ys)), number=100))"
msgstr ""

#: ../../../src/usage/function_transforms.rst:187
msgid ""
"On an M1 Max the naive version takes in total ``0.390`` seconds whereas the "
"vectorized version takes only ``0.025`` seconds, more than ten times faster."
msgstr ""

#: ../../../src/usage/function_transforms.rst:190
msgid ""
"Of course, this operation is quite contrived. A better approach is to simply "
"do ``xs + ys.T``, but for more complex functions :func:`vmap` can be quite "
"handy."
msgstr ""
"當然，這個運算相當刻意。更好的方式是直接做 ``xs + ys.T``，但對更複雜的函式來"
"說，:func:`vmap` 仍然非常實用。"
